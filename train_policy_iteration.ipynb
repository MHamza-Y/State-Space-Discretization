{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from dynamic_programming.policy_iteration import PolicyIteration\n",
    "from dynamic_programming.mdp_model import MDPModel\n",
    "from dynamic_programming.policy import DPPolicy\n",
    "from envs.env_creator import env_creator\n",
    "from base_rl.eval_policy import EvalDiscreteStatePolicy\n",
    "from envs.plot import plot_industrial_benchmark_trajectories\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [20, 12]\n",
    "fixed_digits = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "model_names = ['model_aeq-20bits3']\n",
    "root_path = 'tmp'\n",
    "steps_per_episode = 1000\n",
    "training_episodes = [10, 100, 1000, 10000]\n",
    "total_epochs=10\n",
    "eval_epochs=500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and MDP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.offline_experiment_configs import OfflineDiscreteRLExperimentConfig\n",
    "\n",
    "\n",
    "experiment_configs = []\n",
    "\n",
    "\n",
    "for model_name in model_names:\n",
    "    for i, training_episode in enumerate(training_episodes):\n",
    "        experiment_configs.append(\n",
    "            OfflineDiscreteRLExperimentConfig(\n",
    "                model_name=model_name,\n",
    "                model_path=os.path.join(root_path, 'state_quantization', model_name),\n",
    "                dataset_path=os.path.join(root_path, \"offline_rl_trajectories\", model_name, \"rl_dataset.npy\"),\n",
    "                mdp_path=os.path.join(root_path,'policy_iteration','mdp',model_name,f'{training_episode}','mdp_model.pkl'),\n",
    "                policy_path=os.path.join(root_path,'policy_iteration',model_name, f'{training_episode}','policy.pkl'),\n",
    "                dataset_size=training_episode*steps_per_episode)\n",
    "        )\n",
    "device = 'cpu'\n",
    "reward_type = 'state'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynamic_programming.mdp_model import create_mdp_models\n",
    "\n",
    "\n",
    "for config in experiment_configs:\n",
    "    create_mdp_models(load_path=config.dataset_path, mdp_save_path=config.mdp_path, reward_function_type=reward_type, device=device, dataset_size=config.dataset_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_policy_iteration(mdp_path, policy_save_path):\n",
    "    mdp_model = MDPModel.load(mdp_path)\n",
    "    solver = PolicyIteration(reward_function=mdp_model.reward_function, transition_model=mdp_model.transition_model,\n",
    "                             gamma=0.995, sa_reward=reward_type)\n",
    "    solver.train(total_epochs=total_epochs, eval_epochs=eval_epochs)\n",
    "    trained_policy = DPPolicy(policy_table=solver.policy, state_to_index=mdp_model.state_to_index,\n",
    "                              index_to_action=mdp_model.index_to_actions)\n",
    "    trained_policy.save(policy_save_path)\n",
    "\n",
    "\n",
    "for config in experiment_configs:\n",
    "    train_policy_iteration(mdp_path=config.mdp_path, policy_save_path=config.policy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmarks.policy_benchmarks import PolicyBenchmarks\n",
    "\n",
    "steps_per_episode = 1000\n",
    "evaluators = []\n",
    "for config in experiment_configs:\n",
    "    print(config.policy_path)\n",
    "    eval_policy = config.get_saved_policy()\n",
    "    env_kwargs = {'steps_per_episode': steps_per_episode, 'device': device, 'model_path': config.model_path}\n",
    "    evaluator = EvalDiscreteStatePolicy(policy=eval_policy, env_creator=env_creator, env_kwargs=env_kwargs,\n",
    "                                        tag=f'{config.model_name}/{config.dataset_size}')\n",
    "    evaluators.append(evaluator)\n",
    "\n",
    "policy_benchmarks = PolicyBenchmarks(evaluators=evaluators, epochs=10)\n",
    "policy_benchmarks.benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_industrial_benchmark_trajectories(policy_benchmarks.evaluators[-1].eval_trajectories[0]['info'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(policy_benchmarks.benchmark_metrics)\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [20, 12]\n",
    "\n",
    "\n",
    "for model_name in model_names:\n",
    "    m = np.core.defchararray.find(df.columns.values.astype(str), model_name) >= 0\n",
    "    fdf = df.loc[:, m]\n",
    "    ax = fdf.plot.bar()\n",
    "\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
