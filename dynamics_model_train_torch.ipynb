{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamza/PycharmProjects/StateCompression/dynamic_model/model.py:34: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if layer_idx is 0:\n",
      "/home/hamza/PycharmProjects/StateCompression/dynamic_model/model.py:63: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if layer_idx is 0:\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dynamic_model.dataset import DynamicsModelDataset\n",
    "from dynamic_model.model import DynamicsModel\n",
    "from dynamic_model.train import train_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print(f\"Using Device: {device}\")\n",
    "torch.backends.cudnn.benchmark = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "def load_dataset(file_path, input_key, output_key, test_size= 0.3, device=device):\n",
    "\n",
    "    dataset = np.load(file_path, allow_pickle=True)\n",
    "    x = dataset[()][input_key]\n",
    "    y = dataset[()][output_key]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size)\n",
    "    train_dataset = DynamicsModelDataset(x_train,y_train, device)\n",
    "    validation_dataset = DynamicsModelDataset(x_test,y_test, device)\n",
    "    return train_dataset, validation_dataset\n",
    "\n",
    "\n",
    "dataset_input_key = 'merged_input'\n",
    "dataset_output_key = 'merged_output'\n",
    "dataset_file_path = 'tmp/ib-out/ib-samples.npy'\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 8000\n",
    "params = {'batch_size': batch_size,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0,\n",
    "          'drop_last':True}\n",
    "\n",
    "\n",
    "\n",
    "train_dataset, val_dataset = load_dataset(file_path=dataset_file_path, input_key=dataset_input_key, output_key= dataset_output_key)\n",
    "train_loader = DataLoader(train_dataset, **params)\n",
    "val_loader = DataLoader(val_dataset, **params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "num_of_features = train_dataset.get_features_size()\n",
    "seq_len = train_dataset.get_seq_len()\n",
    "hidden_size = 32\n",
    "out_size = train_dataset.get_output_feature_size()\n",
    "\n",
    "model = DynamicsModel(features=num_of_features,hidden_size=hidden_size,out_size=out_size, batch_size=batch_size, seq_len=seq_len, n_layers=2).to(device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained test\n",
      "--------\n",
      "Test loss: 13143.432291666666\n",
      "\n",
      "Epoch 0\n",
      "---------\n",
      "Train loss: 11857.842087518602\n",
      "Test loss: 10541.911051432291\n",
      "Epoch time: epoch_time = 12.726s\n",
      "Epoch 1\n",
      "---------\n",
      "Train loss: 9772.029610770089\n",
      "Test loss: 8883.919162326389\n",
      "Epoch time: epoch_time = 12.632s\n",
      "Epoch 2\n",
      "---------\n",
      "Train loss: 8412.58485049293\n",
      "Test loss: 7795.26666937934\n",
      "Epoch time: epoch_time = 12.620s\n",
      "Epoch 3\n",
      "---------\n",
      "Train loss: 7461.519804454985\n",
      "Test loss: 7016.9521077473955\n",
      "Epoch time: epoch_time = 12.619s\n",
      "Epoch 4\n",
      "---------\n",
      "Train loss: 6729.555181594122\n",
      "Test loss: 6267.1862386067705\n",
      "Epoch time: epoch_time = 12.614s\n",
      "Epoch 5\n",
      "---------\n",
      "Train loss: 6141.75832984561\n",
      "Test loss: 5791.808281792535\n",
      "Epoch time: epoch_time = 12.591s\n",
      "Epoch 6\n",
      "---------\n",
      "Train loss: 5702.405261811756\n",
      "Test loss: 5433.032660590277\n",
      "Epoch time: epoch_time = 12.695s\n",
      "Epoch 7\n",
      "---------\n",
      "Train loss: 5383.560023716518\n",
      "Test loss: 5150.893079969618\n",
      "Epoch time: epoch_time = 12.811s\n",
      "Epoch 8\n",
      "---------\n",
      "Train loss: 5140.65154157366\n",
      "Test loss: 4945.587660047743\n",
      "Epoch time: epoch_time = 12.721s\n",
      "Epoch 9\n",
      "---------\n",
      "Train loss: 4962.939243861607\n",
      "Test loss: 4794.1985270182295\n",
      "Epoch time: epoch_time = 12.842s\n",
      "Epoch 10\n",
      "---------\n",
      "Train loss: 4839.97073218936\n",
      "Test loss: 4693.445543077257\n",
      "Epoch time: epoch_time = 12.857s\n",
      "Epoch 11\n",
      "---------\n",
      "Train loss: 4749.292311895461\n",
      "Test loss: 4618.736056857639\n",
      "Epoch time: epoch_time = 13.193s\n",
      "Epoch 12\n",
      "---------\n",
      "Train loss: 4682.254307338169\n",
      "Test loss: 4572.704928927951\n",
      "Epoch time: epoch_time = 13.214s\n",
      "Epoch 13\n",
      "---------\n",
      "Train loss: 4647.384599958147\n",
      "Test loss: 4538.495930989583\n",
      "Epoch time: epoch_time = 13.553s\n",
      "Epoch 14\n",
      "---------\n",
      "Train loss: 4617.017156691778\n",
      "Test loss: 4516.64006890191\n",
      "Epoch time: epoch_time = 12.955s\n",
      "Epoch 15\n",
      "---------\n",
      "Train loss: 4604.948829287574\n",
      "Test loss: 4500.737209743924\n",
      "Epoch time: epoch_time = 13.182s\n",
      "Epoch 16\n",
      "---------\n",
      "Train loss: 4589.181937081473\n",
      "Test loss: 4494.121839735243\n",
      "Epoch time: epoch_time = 13.014s\n",
      "Epoch 17\n",
      "---------\n",
      "Train loss: 4585.261739095052\n",
      "Test loss: 4492.932142469618\n",
      "Epoch time: epoch_time = 13.065s\n",
      "Epoch 18\n",
      "---------\n",
      "Train loss: 4584.646432059152\n",
      "Test loss: 4496.460842556424\n",
      "Epoch time: epoch_time = 13.004s\n",
      "Epoch 19\n",
      "---------\n",
      "Train loss: 4577.455432710193\n",
      "Test loss: 4492.568603515625\n",
      "Epoch time: epoch_time = 13.223s\n",
      "Epoch 20\n",
      "---------\n",
      "Train loss: 4578.6000569661455\n",
      "Test loss: 4481.0073649088545\n",
      "Epoch time: epoch_time = 13.199s\n",
      "Epoch 21\n",
      "---------\n",
      "Train loss: 4576.806236630395\n",
      "Test loss: 4489.286539713542\n",
      "Epoch time: epoch_time = 13.102s\n",
      "Epoch 22\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamza/PycharmProjects/StateCompression/dynamic_model/model.py:34: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if layer_idx is 0:\n",
      "/home/hamza/PycharmProjects/StateCompression/dynamic_model/model.py:63: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if layer_idx is 0:\n",
      "/home/hamza/PycharmProjects/StateCompression/dynamic_model/model.py:34: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if layer_idx is 0:\n",
      "/home/hamza/PycharmProjects/StateCompression/dynamic_model/model.py:63: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if layer_idx is 0:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [6]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m----> 2\u001B[0m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1e-2\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/StateCompression/dynamic_model/train.py:53\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, test_loader, train_loader, loss_function, optimizer, learning_rate, n_epochs)\u001B[0m\n\u001B[1;32m     51\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time()\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mix_epoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m---------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 53\u001B[0m \u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_function\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m test_step(test_loader, model, loss_function)\n\u001B[1;32m     55\u001B[0m end_time \u001B[38;5;241m=\u001B[39m time()\n",
      "File \u001B[0;32m~/PycharmProjects/StateCompression/dynamic_model/train.py:12\u001B[0m, in \u001B[0;36mtrain_step\u001B[0;34m(data_loader, model, loss_function, optimizer)\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m#hc = model.init_hidden()\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m X, y \u001B[38;5;129;01min\u001B[39;00m data_loader:\n\u001B[0;32m---> 12\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss_function(output, y)\n\u001B[1;32m     15\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m~/PycharmProjects/StateCompression/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/StateCompression/dynamic_model/model.py:68\u001B[0m, in \u001B[0;36mDynamicsModel.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     66\u001B[0m     layer_input \u001B[38;5;241m=\u001B[39m h[layer_idx \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m---> 68\u001B[0m (h[layer_idx], c[layer_idx]) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlstm_cells\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlayer_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlayer_input\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mh\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlayer_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlayer_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m layer_idx \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_layers \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m     70\u001B[0m     h[layer_idx] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactivation_layers[layer_idx](h[layer_idx])\n",
      "File \u001B[0;32m~/PycharmProjects/StateCompression/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/StateCompression/venv/lib/python3.8/site-packages/torch/nn/modules/rnn.py:1189\u001B[0m, in \u001B[0;36mLSTMCell.forward\u001B[0;34m(self, input, hx)\u001B[0m\n\u001B[1;32m   1186\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1187\u001B[0m     hx \u001B[38;5;241m=\u001B[39m (hx[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m), hx[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_batched \u001B[38;5;28;01melse\u001B[39;00m hx\n\u001B[0;32m-> 1189\u001B[0m ret \u001B[38;5;241m=\u001B[39m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlstm_cell\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1190\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1191\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight_ih\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight_hh\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1192\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias_ih\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias_hh\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1193\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_batched:\n\u001B[1;32m   1196\u001B[0m     ret \u001B[38;5;241m=\u001B[39m (ret[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m0\u001B[39m), ret[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m0\u001B[39m))\n",
      "File \u001B[0;32m/usr/lib/python3.8/traceback.py:197\u001B[0m, in \u001B[0;36mformat_stack\u001B[0;34m(f, limit)\u001B[0m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m f \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    196\u001B[0m     f \u001B[38;5;241m=\u001B[39m sys\u001B[38;5;241m.\u001B[39m_getframe()\u001B[38;5;241m.\u001B[39mf_back\n\u001B[0;32m--> 197\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m format_list(\u001B[43mextract_stack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlimit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlimit\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m/usr/lib/python3.8/traceback.py:211\u001B[0m, in \u001B[0;36mextract_stack\u001B[0;34m(f, limit)\u001B[0m\n\u001B[1;32m    209\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m f \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    210\u001B[0m     f \u001B[38;5;241m=\u001B[39m sys\u001B[38;5;241m.\u001B[39m_getframe()\u001B[38;5;241m.\u001B[39mf_back\n\u001B[0;32m--> 211\u001B[0m stack \u001B[38;5;241m=\u001B[39m \u001B[43mStackSummary\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextract\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwalk_stack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlimit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlimit\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    212\u001B[0m stack\u001B[38;5;241m.\u001B[39mreverse()\n\u001B[1;32m    213\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m stack\n",
      "File \u001B[0;32m/usr/lib/python3.8/traceback.py:362\u001B[0m, in \u001B[0;36mStackSummary.extract\u001B[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001B[0m\n\u001B[1;32m    359\u001B[0m     result\u001B[38;5;241m.\u001B[39mappend(FrameSummary(\n\u001B[1;32m    360\u001B[0m         filename, lineno, name, lookup_line\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;28mlocals\u001B[39m\u001B[38;5;241m=\u001B[39mf_locals))\n\u001B[1;32m    361\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m filename \u001B[38;5;129;01min\u001B[39;00m fnames:\n\u001B[0;32m--> 362\u001B[0m     \u001B[43mlinecache\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheckcache\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    363\u001B[0m \u001B[38;5;66;03m# If immediate lookup was desired, trigger lookups now.\u001B[39;00m\n\u001B[1;32m    364\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m lookup_lines:\n",
      "File \u001B[0;32m~/PycharmProjects/StateCompression/venv/lib/python3.8/site-packages/IPython/core/compilerop.py:193\u001B[0m, in \u001B[0;36mcheck_linecache_ipython\u001B[0;34m(*args)\u001B[0m\n\u001B[1;32m    190\u001B[0m \u001B[38;5;124;03m\"\"\"Call linecache.checkcache() safely protecting our cached values.\u001B[39;00m\n\u001B[1;32m    191\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    192\u001B[0m \u001B[38;5;66;03m# First call the original checkcache as intended\u001B[39;00m\n\u001B[0;32m--> 193\u001B[0m \u001B[43mlinecache\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_checkcache_ori\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    194\u001B[0m \u001B[38;5;66;03m# Then, update back the cache with our data, so that tracebacks related\u001B[39;00m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;66;03m# to our compiled codes can be produced.\u001B[39;00m\n\u001B[1;32m    196\u001B[0m linecache\u001B[38;5;241m.\u001B[39mcache\u001B[38;5;241m.\u001B[39mupdate(linecache\u001B[38;5;241m.\u001B[39m_ipython_cache)\n",
      "File \u001B[0;32m/usr/lib/python3.8/linecache.py:74\u001B[0m, in \u001B[0;36mcheckcache\u001B[0;34m(filename)\u001B[0m\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m   \u001B[38;5;66;03m# no-op for files loaded via a __loader__\u001B[39;00m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 74\u001B[0m     stat \u001B[38;5;241m=\u001B[39m \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfullname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m:\n\u001B[1;32m     76\u001B[0m     cache\u001B[38;5;241m.\u001B[39mpop(filename, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "train_model(model=model, train_loader=train_loader, test_loader=val_loader, n_epochs=10000, learning_rate=1e-2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}