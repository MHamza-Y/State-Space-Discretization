{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ray.tune as tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.tune import register_env\n",
    "from envs.env_creator import env_creator, ibgym_env_creator_rllib\n",
    "from envs.IBGym_mod_envs import IBGymModded\n",
    "from ppo.policy import LSTMPPOPolicy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configure Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "register_env(\"IBGym-v1\", ibgym_env_creator_rllib)\n",
    "\n",
    "config = {\n",
    "    # Environment (RLlib understands openAI gym registered strings).\n",
    "    \"env\": \"IBGym-v1\",\n",
    "    # Use 2 environment workers (aka \"rollout workers\") that parallelly\n",
    "    # collect samples from their own environment clone(s).\n",
    "    \"num_workers\": 12,\n",
    "    \"num_gpus\": 1,\n",
    "\n",
    "    # Change this to \"framework: torch\", if you are using PyTorch.\n",
    "    # Also, use \"framework: tf2\" for tf2.x eager execution.\n",
    "    \"framework\": \"tf\",\n",
    "    \"entropy_coeff\": 0.0001,\n",
    "    # \"entropy_coeff_schedule\":PiecewiseSchedule(endpoints=[(0, 0.01), (143000, 0.00001)]),\n",
    "    \"lr\": 3e-5,\n",
    "    \"gamma\": 0.994,\n",
    "    \"clip_param\": 0.2,\n",
    "    \"seed\": 5321,\n",
    "    \"num_sgd_iter\": 2,\n",
    "    \"sgd_minibatch_size\": 1000,\n",
    "\n",
    "    # \"vf_loss_coeff\": 1e-9,\n",
    "    # \"vf_clip_param\": 1e7,\n",
    "    # Tweak the default model provided automatically by RLlib,\n",
    "    # given the environment's observation- and action spaces.\n",
    "    \"model\": {\n",
    "        # == LSTM ==\n",
    "        # Whether to wrap the model with an LSTM.\n",
    "        \"use_lstm\": True,\n",
    "        # Max seq len for training the LSTM, defaults to 20.\n",
    "        \"max_seq_len\": 30,\n",
    "        # Size of the LSTM cell.\n",
    "        \"lstm_cell_size\": 64,\n",
    "        # \"use_attention\": True,\n",
    "        # \"attention_num_transformer_units\": 2,\n",
    "        # \"attention_dim\": 128,\n",
    "        # \"vf_share_layers\": True,\n",
    "        # \"fcnet_hiddens\": [32, 32, 32],\n",
    "        # \"sgd_minibatch_size\": 1024,\n",
    "        \"vf_share_layers\": False,\n",
    "        # Whether to feed a_{t-1} to LSTM (one-hot encoded if discrete).\n",
    "        \"lstm_use_prev_action\": False,\n",
    "        # Whether to feed r_{t-1} to LSTM.\n",
    "        \"lstm_use_prev_reward\": False,\n",
    "        # Whether the LSTM is time-major (TxBx..) or batch-major (BxTx..).\n",
    "        \"_time_major\": False,\n",
    "    },\n",
    "    \"train_batch_size\": 32000,\n",
    "    \"timesteps_per_iteration\": 32000,\n",
    "    # \"output\": \"tmp/ib-out\",\n",
    "    # Set up a separate evaluation worker set for the\n",
    "    # `trainer.evaluate()` call after training (see below).\n",
    "    \"evaluation_num_workers\": 3,\n",
    "    # Only for evaluation runs, render the env.\n",
    "    \"evaluation_config\": {\n",
    "        \"render_env\": False,\n",
    "    },\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = tune.run(\n",
    "        PPOTrainer,\n",
    "        config=config,\n",
    "        name=\"industrial_benchmark\",\n",
    "        local_dir=\"tmp/ray_exp_logs\",\n",
    "        checkpoint_freq=5,\n",
    "        # stop={\"training_iteration\": 5},\n",
    "        sync_config=tune.SyncConfig(\n",
    "            syncer=None  # Disable syncing\n",
    "        )\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "last_checkpoint = results.get_last_checkpoint()._local_path\n",
    "config[\"num_workers\"] = 1\n",
    "\n",
    "lstm_ppo_policy = LSTMPPOPolicy(config=config,checkpoint_path=last_checkpoint)\n",
    "save_path = 'ppo/lstm_ppo_policy.pkl'\n",
    "lstm_ppo_policy.save(save_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results.get_last_checkpoint()._local_path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
