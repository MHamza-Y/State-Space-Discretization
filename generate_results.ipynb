{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import mannwhitneyu\n",
    "import seaborn as sns\n",
    "from itertools import combinations, permutations\n",
    "import numpy as np\n",
    "import warnings\n",
    "from os.path import join, dirname\n",
    "from os import makedirs\n",
    "from matplotlib import pyplot as plt\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "\n",
    "pd.set_option('display.float_format', '{:.6g}'.format)\n",
    "df_save_path = 'tmp/evaluation_results'\n",
    "\n",
    "results_df = pd.read_pickle(df_save_path).fillna(np.nan)\n",
    "results_df['algo'] = results_df['algo'].replace(['q_learning','mb_q_learning','policy_iteration','rmin'],['Q Learning', 'Model-Based Q Learning', 'Policy Iteration', 'R-Min'])\n",
    "results_df['model_name'] = results_df['model_name'].replace(['model_final_h-20bits','model_aeq-20bits','model_h_c-20bits','untrained_model_final_h-20bits','untrained_model_aeq-20bits','untrained_model_h_c-20bits',],['DiscFinalH', 'DiscLSTM-AE', 'DiscHC', 'Untrained DiscFinalH', 'Untrained DiscLSTM-AE', 'Untrained DiscHC'])\n",
    "results_df = results_df[(results_df['save_type'] == 'x_interval') | (results_df['save_type'].isna())]\n",
    "algos = results_df['algo'].unique().tolist()\n",
    "results_df['log_model_loss'] = np.log10(results_df['model_loss'])\n",
    "trained_results_df = results_df[results_df['trained_model']==True]\n",
    "const_dataset_results_df = results_df[(results_df['dataset_size']==10000)|(results_df['dataset_size'].isnull())]\n",
    "const_dataset_trained_results_df = const_dataset_results_df[const_dataset_results_df['trained_model']==True]\n",
    "results_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_fig(plot_obj, fig_save_path):\n",
    "    makedirs(dirname(fig_save_path), exist_ok=True)\n",
    "    plot_obj.savefig(fig_save_path,bbox_inches = 'tight')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_bar_graph(df,x,y='mean',yerr_idx='std',ylim = None, figsize=None, save_path=None,**kwargs):\n",
    "    figsize = figsize if figsize else (12, 7)\n",
    "    plt.figure(figsize=figsize)\n",
    "    ax = sns.barplot(x=x, y=y, data=df,**kwargs)\n",
    "    if ylim:\n",
    "        ax.set_ylim(ylim)\n",
    "    x_coords = [p.get_x() + 0.5 * p.get_width() for p in ax.patches]\n",
    "    y_coords = [p.get_height() for p in ax.patches]\n",
    "    ax.errorbar(x=x_coords, y=y_coords, yerr=df[yerr_idx], fmt=\"none\", c=\"k\", capsize=8)\n",
    "    if save_path:\n",
    "        save_fig(ax.get_figure(),save_path)\n",
    "\n",
    "def box_plot(df,x,y='mean',ylim = None, xlim=None,figsize=None, save_path=None, major_ticks=np.arange(-2000, -150, 200), minor_ticks = np.arange(-2000, -150, 100),legend_title=None,**kwargs):\n",
    "    df = df.copy()\n",
    "    if df[y].dtypes.name == 'bool':\n",
    "        df[y] = df[y].map({True: 'True', False: 'False'})\n",
    "    figsize = figsize if figsize else (12, 7)\n",
    "    plt.figure(figsize=figsize)\n",
    "    ax = sns.boxplot(x=x, y=y, data=df,**kwargs)\n",
    "    if ylim:\n",
    "        ax.set_ylim(ylim)\n",
    "    if xlim:\n",
    "        ax.set_xlim(xlim)\n",
    "\n",
    "\n",
    "    ax.set_xticks(major_ticks)\n",
    "    ax.set_xticks(minor_ticks, minor=True)\n",
    "\n",
    "\n",
    "    ax.grid(which='minor', alpha=0.2)\n",
    "    ax.grid(which='major', alpha=0.5)\n",
    "    leg = plt.legend()\n",
    "    for lh in leg.legendHandles:\n",
    "        lh.set_alpha(0.3)\n",
    "\n",
    "    if legend_title:\n",
    "        ax.get_legend().set_title(legend_title)\n",
    "    if save_path:\n",
    "        save_fig(ax.get_figure(),save_path)\n",
    "\n",
    "def dist_plot(df,x,y='mean',ylim = None, xlim=None,figsize=None, save_path=None, major_ticks=np.arange(-2000, -150, 200), minor_ticks = np.arange(-2000, -150, 100),legend_title=None,**kwargs):\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    figsize = figsize if figsize else (12, 7)\n",
    "    plt.figure(figsize=figsize)\n",
    "    ax = sns.displot(x=x, y=y, data=df,**kwargs)\n",
    "    if ylim:\n",
    "        ax.set_ylim(ylim)\n",
    "    if xlim:\n",
    "        ax.set_xlim(xlim)\n",
    "\n",
    "\n",
    "    ax.set_xticks(major_ticks)\n",
    "    ax.set_xticks(minor_ticks, minor=True)\n",
    "\n",
    "\n",
    "    ax.grid(which='minor', alpha=0.2)\n",
    "    ax.grid(which='major', alpha=0.5)\n",
    "    leg = plt.legend()\n",
    "    for lh in leg.legendHandles:\n",
    "        lh.set_alpha(0.3)\n",
    "\n",
    "    if legend_title:\n",
    "        ax.get_legend().set_title(legend_title)\n",
    "    if save_path:\n",
    "        save_fig(ax.get_figure(),save_path)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test If Training helps\n",
    "$H_{0}$: Training the discretization model has no effect on the final rewards\n",
    "$H_{a}$: Training the discretization model results in greater rewards"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_hypothesis_columns = ['algo','x','y','p-value']\n",
    "training_hypothesis_df = pd.DataFrame(columns=training_hypothesis_columns)\n",
    "for algo in results_df['algo'].unique():\n",
    "\n",
    "    trained_rewards = const_dataset_results_df[const_dataset_results_df['algo']==algo][const_dataset_results_df['trained_model']==True]['rewards'].tolist()\n",
    "    untrained_rewards = const_dataset_results_df[const_dataset_results_df['algo']==algo][const_dataset_results_df['trained_model']==False]['rewards'].tolist()\n",
    "    #sns_plot = sns.displot(const_dataset_results_df[const_dataset_results_df['algo']==algo], x=\"rewards\", hue=\"trained_model\", kind=\"kde\", fill=True, label=algo)\n",
    "\n",
    "    #save_fig(sns_plot,fig_save_path)\n",
    "    corr, p_value = mannwhitneyu(trained_rewards,untrained_rewards, alternative='greater')\n",
    "    new_row = pd.DataFrame([[algo,True,False,p_value]], columns=training_hypothesis_columns )\n",
    "    training_hypothesis_df = pd.concat((training_hypothesis_df,new_row))\n",
    "print(training_hypothesis_df)\n",
    "fig_save_path = join('tmp','results','training_test')\n",
    "box_plot(df=const_dataset_results_df, x=\"rewards\", y=\"algo\",hue='trained_model',xlim=[-2000,-100],save_path=fig_save_path,boxprops=dict(alpha=.3),figsize=(10,12),legend_title='Model Trained?')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "algo_combinations = list(permutations(algos,2))\n",
    "algo_combinations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test if results from one algorithm is better than other\n",
    "$H_{0}$: Algorithm A results in rewards same as B\n",
    "$H_{a}$: Algorithm A results in rewards greater then B"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "algo_comp_hypothesis_columns = ['algo_A','algo_B','p-value']\n",
    "algo_comp_hypothesis_df = pd.DataFrame(columns=algo_comp_hypothesis_columns)\n",
    "for algo_combination in algo_combinations:\n",
    "    print(algo_combination)\n",
    "    trained_rewards_algo = [const_dataset_results_df[const_dataset_results_df['algo']==algo][const_dataset_results_df['trained_model']==True]['rewards'].tolist() for algo in algo_combination]\n",
    "\n",
    "\n",
    "\n",
    "    _, p_value = mannwhitneyu(trained_rewards_algo[0],trained_rewards_algo[1], alternative='greater')\n",
    "    new_row = pd.DataFrame([[algo_combination[0],algo_combination[1],p_value]], columns=algo_comp_hypothesis_columns )\n",
    "    algo_comp_hypothesis_df = pd.concat((algo_comp_hypothesis_df,new_row))\n",
    "print(algo_comp_hypothesis_df)\n",
    "print(algo_comp_hypothesis_df.reset_index(drop=True).to_latex())\n",
    "fig_save_path = join('tmp','results','algo_compare')\n",
    "box_plot(df=const_dataset_results_df[const_dataset_results_df['trained_model']==True],x=\"rewards\",y='algo',figsize=(7,12),boxprops=dict(alpha=.3),save_path=fig_save_path,major_ticks=np.arange(-1000, -150, 100), minor_ticks = np.arange(-1000, -150, 50))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_corr(df,x,y):\n",
    "    col = ['algo','corr','p_value']\n",
    "    results_df = pd.DataFrame(columns=col)\n",
    "    for algo in df['algo'].unique():\n",
    "        corr, p_value = scipy.stats.kendalltau(df[df['algo']==algo][x].tolist(), df[df['algo']==algo][y].tolist())\n",
    "        new_row = pd.DataFrame([[algo,corr,p_value]], columns=col )\n",
    "        results_df = pd.concat((results_df,new_row))\n",
    "    return results_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Correlation between rewards and dataset size\n",
    "$H_{0}$: Rewards are independent from the dataset size used for the offline algorithms\n",
    "$H_{a}$: Rewards are dependent on the dataset size used for the offline algorithms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "offline_algos = ['R-Min', 'Policy Iteration']\n",
    "offline_trained_df = trained_results_df[trained_results_df['algo'].isin(offline_algos)]\n",
    "\n",
    "dataset_size_corr_df = calculate_corr(offline_trained_df,'dataset_size','rewards')\n",
    "fig_save_path = join('tmp','results','dataset_compare')\n",
    "box_plot(df=offline_trained_df,x=\"rewards\",y='algo',hue='dataset_size',figsize=(7,12),boxprops=dict(alpha=.3),save_path=fig_save_path,major_ticks=np.arange(-1000, -150, 100), minor_ticks = np.arange(-1000, -150, 50),legend_title='Dataset Size (Episodes)')\n",
    "#print(dataset_size_corr_df)\n",
    "print(dataset_size_corr_df.reset_index(drop=True).to_latex())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "offline_trained_df[offline_trained_df['algo']=='rmin']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Correlation between rewards and model loss\n",
    "$H_{0}$: Rewards are independent from the loss of the discretizing model\n",
    "$H_{a}$: Rewards are dependent on the loss of the discretizing model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "loss_test_df = const_dataset_trained_results_df[const_dataset_trained_results_df['dataset_size'].isin([np.nan,10000])]\n",
    "model_loss_corr_df = calculate_corr(loss_test_df,'model_loss','rewards')\n",
    "\n",
    "print(model_loss_corr_df)\n",
    "print(model_loss_corr_df.to_latex())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Correlation between rewards and log model loss\n",
    "$H_{0}$: Rewards are independent from the log loss of the discretizing model\n",
    "$H_{a}$: Rewards are dependent on the log loss of the discretizing model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "log_model_loss_corr_df = calculate_corr(loss_test_df,'log_model_loss','rewards')\n",
    "print(model_loss_corr_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Correlation between rewards and total_states\n",
    "$H_{0}$: Rewards are independent from the total states in the policy\n",
    "$H_{a}$: Rewards are dependent on the total states in the policy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for model in loss_test_df['model_name'].unique():\n",
    "    print(model)\n",
    "    total_states_corr_df = calculate_corr(loss_test_df[loss_test_df['model_name']==model],'total_states', 'rewards')\n",
    "    #print(total_states_corr_df)\n",
    "    print(total_states_corr_df.to_latex())\n",
    "box_plot(loss_test_df,x='total_states',y='model_name', major_ticks=np.arange(0, 8000, 500), minor_ticks = np.arange(0, 8000, 250),save_path=join('tmp','results','total-states-dist'),boxprops=dict(alpha=.3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Correlation between rewards and observed states\n",
    "$H_{0}$: Rewards are independent from the observed states during evaluation\n",
    "$H_{a}$: Rewards are dependent on the observed states during evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "observed_states_corr_df = calculate_corr(loss_test_df,'unique_obs', 'rewards')\n",
    "print(observed_states_corr_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for model in results_df['model_name'].unique():\n",
    "    mean_loss_df = results_df.groupby('model_name')['model_loss'].agg(['mean','std']).reset_index()\n",
    "mean_loss_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "box_plot(const_dataset_results_df,x='new_unique_obs',y='algo',hue='model_name',major_ticks=np.arange(0, 200, 10), minor_ticks = np.arange(0, 200, 5),save_path=join('tmp','results','new-unique-obs-dist'),boxprops=dict(alpha=.3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Test if results from one model are better than other\n",
    "$H_{0}$: Model A results in rewards same as B\n",
    "$H_{a}$: Model A results in rewards greater then B"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_comp_hypothesis_columns = ['Model A','Model B','p-Value']\n",
    "model_comp_hypothesis_df = pd.DataFrame(columns=algo_comp_hypothesis_columns)\n",
    "models = const_dataset_trained_results_df['model_name'].unique()\n",
    "model_permutations = list(permutations(models,2))\n",
    "for model_combination in model_permutations:\n",
    "    trained_rewards_algo1 = [const_dataset_trained_results_df[const_dataset_trained_results_df['model_name']==model]['rewards'].tolist() for model in model_combination]\n",
    "\n",
    "    _, p_value = mannwhitneyu(trained_rewards_algo1[0],trained_rewards_algo1[1], alternative='greater')\n",
    "    new_row = pd.DataFrame([[model_combination[0],model_combination[1],p_value]], columns=model_comp_hypothesis_columns )\n",
    "    model_comp_hypothesis_df = pd.concat((model_comp_hypothesis_df,new_row))\n",
    "\n",
    "\n",
    "print(model_comp_hypothesis_df.to_latex())\n",
    "box_plot(const_dataset_trained_results_df,x='rewards',y='algo',hue='model_name',major_ticks=np.arange(-1000, -150, 100), minor_ticks = np.arange(-1000, -150, 50),legend_title='Discretization Models',save_path=join('tmp','results','model-reward-dist'),boxprops=dict(alpha=.3),figsize=(7,12))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "print(const_dataset_trained_results_df.groupby(['exp_id','model_name','algo']).agg('mean').reset_index().groupby(['model_name','algo']).agg('max').reset_index().to_latex())\n",
    "const_dataset_trained_results_df.groupby(['exp_id','model_name','algo']).agg('mean').reset_index().groupby(['model_name','algo']).agg('max').reset_index()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Mean Algo rewards per experiment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "exp_algo_grouped_mean_rewards_df = trained_results_df.groupby(['exp_id','algo','model_name','dataset_size'],dropna=False)['rewards'].agg(['mean','std','max','count']).reset_index()\n",
    "\n",
    "exp_algo_grouped_mean_rewards_red_df = exp_algo_grouped_mean_rewards_df.sort_values('mean').drop_duplicates(['algo'],keep='last')\n",
    "\n",
    "ppo_df = pd.DataFrame([[np.nan,'PPO',np.nan,np.nan,-185.24,0.88, np.nan,np.nan]],columns=['exp_id', 'algo', 'model_name', 'dataset_size', 'mean', 'std', 'max',\n",
    "       'count'])\n",
    "exp_algo_grouped_mean_rewards_red_df = exp_algo_grouped_mean_rewards_red_df.append(ppo_df)\n",
    "exp_algo_grouped_mean_rewards_red_df = exp_algo_grouped_mean_rewards_red_df.sort_values(['mean'])\n",
    "plot_bar_graph(df=exp_algo_grouped_mean_rewards_red_df,x='algo', ylim=[-230, -175],save_path=join('tmp','results','algo_best_rewards'), alpha=.3)\n",
    "\n",
    "exp_algo_grouped_mean_rewards_red_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results_df.groupby(['exp_id','model_name','dataset_size'])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "box plot of training variances"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp_algo_grouped_mean_rewards_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "const_dataset_trained_results_df.groupby(['algo','model_name']).agg('mean').reset_index()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tmp_df = const_dataset_trained_results_df.copy()\n",
    "tmp_df['exp_id'] = tmp_df['exp_id'].astype('category').cat.codes+1\n",
    "save_path = join('tmp','results','training_variations')\n",
    "box_plot(df=tmp_df,x='rewards',y='algo',hue='model_name' ,major_ticks=np.arange(-1000, -150, 100), minor_ticks = np.arange(-1000, -150, 50),boxprops=dict(alpha=.3),figsize=(8,17),legend_title='Discrete Model')\n",
    "# for algo in algos:\n",
    "#     print(algo)\n",
    "#     save_path = join('tmp','results','training_variations',f'{algo} variation')\n",
    "#     box_plot(df=tmp_df[tmp_df['algo']==algo],x='rewards',y='model_name' ,save_path=save_path,boxprops=dict(alpha=.3))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "untrained_model_final_h-20bits\n",
      "R-Min\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tmp_df[tmp_df['algo']=='Q Learning']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
