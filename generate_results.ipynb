{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import mannwhitneyu\n",
    "import seaborn as sns\n",
    "from itertools import combinations, permutations\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pd.set_option('display.float_format', '{:.6g}'.format)\n",
    "df_save_path = 'tmp/evaluation_results'\n",
    "\n",
    "results_df = pd.read_pickle(df_save_path).fillna(np.nan)\n",
    "results_df = results_df[(results_df['save_type'] == 'x_interval') | (results_df['save_type'].isna())]\n",
    "algos = results_df['algo'].unique().tolist()\n",
    "trained_results_df = results_df[results_df['trained_model']==True]\n",
    "results_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test If Training helps\n",
    "$H_{0}$: Training the discretization model has no effect on the final rewards\n",
    "$H_{a}$: Training the discretization model results in greater rewards"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_hypothesis_columns = ['algo','x','y','p-value']\n",
    "training_hypothesis_df = pd.DataFrame(columns=training_hypothesis_columns)\n",
    "for algo in results_df['algo'].unique():\n",
    "\n",
    "    trained_rewards = results_df[results_df['algo']==algo][results_df['trained_model']==True]['rewards'].tolist()\n",
    "    untrained_rewards = results_df[results_df['algo']==algo][results_df['trained_model']==False]['rewards'].tolist()\n",
    "    sns.displot(results_df[results_df['algo']==algo], x=\"rewards\", hue=\"trained_model\", kind=\"kde\", fill=True, label=algo)\n",
    "    corr, p_value = mannwhitneyu(trained_rewards,untrained_rewards, alternative='greater')\n",
    "    new_row = pd.DataFrame([[algo,True,False,p_value]], columns=training_hypothesis_columns )\n",
    "    training_hypothesis_df = pd.concat((training_hypothesis_df,new_row))\n",
    "print(training_hypothesis_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "algo_combinations = list(permutations(algos,2))\n",
    "algo_combinations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test if results from one algorithm is better than other\n",
    "$H_{0}$: Algorithm A results in rewards same as B\n",
    "$H_{a}$: Algorithm A results in rewards greater then B"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "algo_comp_hypothesis_columns = ['algo_A','algo_B','p-value']\n",
    "algo_comp_hypothesis_df = pd.DataFrame(columns=algo_comp_hypothesis_columns)\n",
    "for algo_combination in algo_combinations:\n",
    "    print(algo_combination)\n",
    "    trained_rewards_algo = [results_df[results_df['algo']==algo][results_df['trained_model']==True]['rewards'].tolist() for algo in algo_combination]\n",
    "\n",
    "    sns.displot(results_df[(results_df['trained_model']==True) & (results_df['algo'].isin(algo_combination))], x=\"rewards\", hue=\"algo\", kind=\"kde\", fill=True)\n",
    "    _, p_value = mannwhitneyu(trained_rewards_algo[0],trained_rewards_algo[1], alternative='greater')\n",
    "    new_row = pd.DataFrame([[algo_combination[0],algo_combination[1],p_value]], columns=algo_comp_hypothesis_columns )\n",
    "    algo_comp_hypothesis_df = pd.concat((algo_comp_hypothesis_df,new_row))\n",
    "print(algo_comp_hypothesis_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_corr(df,x,y):\n",
    "    col = ['algo','corr','p_value']\n",
    "    results_df = pd.DataFrame(columns=col)\n",
    "    for algo in df['algo'].unique():\n",
    "        corr, p_value = scipy.stats.kendalltau(df[df['algo']==algo][x].tolist(), df[df['algo']==algo][y].tolist())\n",
    "        new_row = pd.DataFrame([[algo,corr,p_value]], columns=col )\n",
    "        results_df = pd.concat((results_df,new_row))\n",
    "    return results_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Correlation between rewards and dataset size\n",
    "$H_{0}$: Rewards are independent from the dataset size used for the offline algorithms\n",
    "$H_{a}$: Rewards are dependent on the dataset size used for the offline algorithms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "offline_algos = ['rmin', 'policy_iteration']\n",
    "offline_trained_df = trained_results_df[trained_results_df['algo'].isin(offline_algos)]\n",
    "\n",
    "dataset_size_corr_df = calculate_corr(offline_trained_df,'dataset_size','rewards')\n",
    "print(dataset_size_corr_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "offline_trained_df[offline_trained_df['algo']=='rmin']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Correlation between rewards and model loss\n",
    "$H_{0}$: Rewards are independent from the loss of the discretizing model\n",
    "$H_{a}$: Rewards are dependent on the loss of the discretizing model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "loss_test_df = trained_results_df[trained_results_df['dataset_size'].isin([np.nan,10000])]\n",
    "model_loss_corr_df = calculate_corr(loss_test_df,'model_loss','rewards')\n",
    "print(model_loss_corr_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Correlation between rewards and total_states\n",
    "$H_{0}$: Rewards are independent from the total states in the policy\n",
    "$H_{a}$: Rewards are dependent on the total states in the policy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total_states_corr_df = calculate_corr(loss_test_df,'total_states', 'rewards')\n",
    "print(total_states_corr_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss_test_df[loss_test_df['algo']=='q_learning']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Correlation between rewards and observed states\n",
    "$H_{0}$: Rewards are independent from the observed states during evaluation\n",
    "$H_{a}$: Rewards are dependent on the observed states during evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "observed_states_corr_df = calculate_corr(loss_test_df,'unique_obs', 'rewards')\n",
    "print(observed_states_corr_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "log of model loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "box plot"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
