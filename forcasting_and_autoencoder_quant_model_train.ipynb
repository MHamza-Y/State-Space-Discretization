{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from state_quantization.dataset import load_dataset\n",
    "from state_quantization.dataset import DynamicsModelDataset\n",
    "from state_quantization.forcasting_models import LSTMForcasting\n",
    "from state_quantization.quantization_models import DiscAutoEncoder\n",
    "from state_quantization.forcasting_quantization_models import ForcastingQuant\n",
    "from state_quantization.trainer import ForcastingQuantTrainer\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from state_quantization.train import train_model, test_step\n",
    "from state_quantization.eval import eval_model, compare_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print(f\"Using Device: {device}\")\n",
    "torch.backends.cudnn.benchmark = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clipping y\n",
      "torch.Size([672000, 39, 6])\n",
      "torch.Size([672000, 10, 2])\n",
      "torch.Size([288000, 39, 6])\n",
      "torch.Size([288000, 10, 2])\n",
      "{'batch_size': 8000, 'shuffle': True, 'num_workers': 0, 'drop_last': True, 'pin_memory': True}\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[-0.9307,  0.4514],\n        [-0.9254,  0.4017],\n        [-1.0641,  0.3700],\n        [-1.0641,  0.3580],\n        [-0.9195,  0.4085],\n        [-0.9129,  0.4301],\n        [-0.8791,  0.3999],\n        [-1.0641,  0.3301],\n        [-1.0641,  0.3159],\n        [-0.9353,  0.3056]])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_input_key = 'merged_input'\n",
    "dataset_output_key = 'merged_output'\n",
    "dataset_file_path = 'tmp/ib-out/ib-samples-la.npy'\n",
    "normalized_data_params_save_path = 'state_quantization/NormalizeInputConfigs.pkl'\n",
    "dataset_device = 'cpu'\n",
    "y_indexes = [4, 6]\n",
    "\n",
    "train_dataset, val_dataset = load_dataset(file_path=dataset_file_path, input_key=dataset_input_key,\n",
    "                                          output_key=dataset_output_key, dataset_class=DynamicsModelDataset,\n",
    "                                          normalize=True, device=dataset_device, y_clip_range=y_indexes,\n",
    "                                          normalized_data_params_save_path=normalized_data_params_save_path)\n",
    "\n",
    "batch_size = 8000\n",
    "params = {'batch_size': batch_size,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0,\n",
    "          'drop_last': True,\n",
    "          'pin_memory': not train_dataset.x.is_cuda}\n",
    "print(params)\n",
    "train_loader = DataLoader(train_dataset, **params)\n",
    "val_loader = DataLoader(val_dataset, **params)\n",
    "train_dataset.y[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out Size:2\n",
      "LSTM Layers\n",
      "ModuleList(\n",
      "  (0): LSTMCell(6, 16)\n",
      ")\n",
      "LSTM Dropout Layers\n",
      "ModuleList()\n",
      "Fully Connected Layers\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (1): GELU(approximate=none)\n",
      "  (2): Dropout(p=0.2, inplace=False)\n",
      "  (3): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (4): GELU(approximate=none)\n",
      "  (5): Dropout(p=0.2, inplace=False)\n",
      "  (6): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n",
      "[32, 16]\n",
      "[16, 32]\n",
      "Encoder Layers\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (1): GELU(approximate=none)\n",
      "  (2): Dropout(p=0.2, inplace=False)\n",
      "  (3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (4): GELU(approximate=none)\n",
      "  (5): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Bottleneck Layers\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=16, out_features=10, bias=True)\n",
      "  (1): StraightThroughEstimator()\n",
      ")\n",
      "Decoded Layers\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=10, out_features=16, bias=True)\n",
      "  (1): GELU(approximate=none)\n",
      "  (2): Dropout(p=0.2, inplace=False)\n",
      "  (3): Linear(in_features=16, out_features=32, bias=True)\n",
      "  (4): GELU(approximate=none)\n",
      "  (5): Dropout(p=0.2, inplace=False)\n",
      "  (6): Linear(in_features=32, out_features=32, bias=True)\n",
      ")\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "model_path = 'state_quantization/model'\n",
    "\n",
    "num_of_features = train_dataset.get_features_size()\n",
    "seq_len = train_dataset.get_seq_len()\n",
    "hidden_size = 16\n",
    "out_size = train_dataset.get_output_feature_size()\n",
    "print(f'Out Size:{out_size}')\n",
    "look_ahead = train_dataset.get_look_ahead_size()\n",
    "n_layers = 1\n",
    "dropout = 0.2\n",
    "\n",
    "forcasting_model = LSTMForcasting(features=num_of_features, hidden_size=hidden_size, out_size=out_size, seq_len=seq_len,\n",
    "                                  look_ahead=look_ahead, dropout=dropout, n_layers=n_layers)\n",
    "\n",
    "disc_autoencoder_input_size = hidden_size * 2\n",
    "bottleneck_size = 10\n",
    "ae_dropout = 0.2\n",
    "disc_autoencoder = DiscAutoEncoder(input_size=disc_autoencoder_input_size, bottleneck_size=bottleneck_size,\n",
    "                                   dropout=ae_dropout)\n",
    "\n",
    "model = ForcastingQuant(forcasting_model=forcasting_model, autoencoder_quant_model=disc_autoencoder).to(device=device)\n",
    "\n",
    "load_to_gpu = model.is_cuda() and not train_dataset.x.is_cuda\n",
    "print(load_to_gpu)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Trainer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "gamma = 0.1\n",
    "\n",
    "forecasting_learning_rate = 1e-3\n",
    "autoencoder_learning_rate = 1e-3\n",
    "\n",
    "forecasting_lr_milestones = [90]\n",
    "autoencoder_lr_milestones = [35]\n",
    "forecasting_optimizer = torch.optim.Adam(model.forcasting_model.parameters(),\n",
    "                                         lr=forecasting_learning_rate)\n",
    "autoencoder_optimizer = torch.optim.Adam(model.autoencoder_quant_model.parameters(),\n",
    "                                         lr=autoencoder_learning_rate)\n",
    "forecasting_lr_scheduler = MultiStepLR(forecasting_optimizer, milestones=forecasting_lr_milestones, gamma=gamma)\n",
    "autoencoder_lr_scheduler = MultiStepLR(autoencoder_optimizer, milestones=autoencoder_lr_milestones, gamma=gamma)\n",
    "\n",
    "trainer = ForcastingQuantTrainer(forcasting_quant_model=model, train_loader=train_loader, test_loader=val_loader,\n",
    "                                 load_to_gpu=load_to_gpu, forecasting_optimizer=forecasting_optimizer,\n",
    "                                 forecasting_lr_scheduler=forecasting_lr_scheduler,\n",
    "                                 autoencoder_lr_scheduler=autoencoder_lr_scheduler,\n",
    "                                 autoencoder_optimizer=autoencoder_optimizer)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained test\n",
      "--------\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 1.01047596666548\n",
      "Autoencoder Test loss: 0.09716833237972525\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 1\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.8297108056999388\n",
      "Autoencoder Train loss: 0.2127241369869028\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.4643946381078826\n",
      "Autoencoder Test loss: 0.3201235623823272\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 8.284s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 2\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.3210209256836346\n",
      "Autoencoder Train loss: 0.21653443432989575\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.1277384561383062\n",
      "Autoencoder Test loss: 0.13479670468303892\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 8.009s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 3\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.16679728421426954\n",
      "Autoencoder Train loss: 0.11696445001732736\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.08420324636002381\n",
      "Autoencoder Test loss: 0.09552770459817515\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 8.093s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 4\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.13894872172247796\n",
      "Autoencoder Train loss: 0.09660919933092027\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.06827733541528384\n",
      "Autoencoder Test loss: 0.08240947148038281\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 7.979s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 5\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.12419126848024982\n",
      "Autoencoder Train loss: 0.08245095832362062\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.060433748074703746\n",
      "Autoencoder Test loss: 0.06999836737910907\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 8.326s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 6\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.11616230427864052\n",
      "Autoencoder Train loss: 0.07472129760398752\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.05577408894896507\n",
      "Autoencoder Test loss: 0.0648748433838288\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 8.088s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 7\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.11030713122870241\n",
      "Autoencoder Train loss: 0.0710727691295601\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.050236245410309896\n",
      "Autoencoder Test loss: 0.06372779235243797\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 8.674s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 8\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.1057400463947228\n",
      "Autoencoder Train loss: 0.06969435663805121\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.046842971816658974\n",
      "Autoencoder Test loss: 0.06351117841485474\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 8.645s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 9\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.10253950563215074\n",
      "Autoencoder Train loss: 0.06870612342442785\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.04473123295853535\n",
      "Autoencoder Test loss: 0.06306426930758688\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 8.437s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 10\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.09975177484254043\n",
      "Autoencoder Train loss: 0.06767488137951919\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.04306631887124644\n",
      "Autoencoder Test loss: 0.06254211771819326\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 8.661s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 11\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.09757438772136257\n",
      "Autoencoder Train loss: 0.06672506433512483\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.04273455207132631\n",
      "Autoencoder Test loss: 0.06112030893564224\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 8.333s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 12\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.09615760092579183\n",
      "Autoencoder Train loss: 0.06553828751757032\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.040492284008198313\n",
      "Autoencoder Test loss: 0.06064810283068153\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 8.985s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 13\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.0948033036575431\n",
      "Autoencoder Train loss: 0.0647809671770249\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.040304217487573624\n",
      "Autoencoder Test loss: 0.05999783498959409\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 8.616s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 14\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.09356748632022313\n",
      "Autoencoder Train loss: 0.06442707253708727\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.038745245378878385\n",
      "Autoencoder Test loss: 0.05997687402284808\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 8.489s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 15\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.09220143354364804\n",
      "Autoencoder Train loss: 0.06421431145142942\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.03881129674199554\n",
      "Autoencoder Test loss: 0.05960031826463011\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 8.377s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 16\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.0911719992402054\n",
      "Autoencoder Train loss: 0.0642615742537947\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.03825933807012108\n",
      "Autoencoder Test loss: 0.05899538637863265\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 8.249s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 17\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.09035057920430388\n",
      "Autoencoder Train loss: 0.06323984402808405\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.03717618715018034\n",
      "Autoencoder Test loss: 0.057161954438520804\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 8.074s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 18\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.0890528191590593\n",
      "Autoencoder Train loss: 0.06117700497131972\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.036627337543500796\n",
      "Autoencoder Test loss: 0.05525680476178726\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 8.279s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 19\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.08809909747824782\n",
      "Autoencoder Train loss: 0.05914994573131913\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.03653900304602252\n",
      "Autoencoder Test loss: 0.05331304079542557\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 7.909s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 20\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.08733024004669417\n",
      "Autoencoder Train loss: 0.05769509121420838\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.03622674486703343\n",
      "Autoencoder Test loss: 0.05188502754188246\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 7.419s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 21\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.08625715181586288\n",
      "Autoencoder Train loss: 0.05628326087303105\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.03512554212162892\n",
      "Autoencoder Test loss: 0.051157258450984955\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 7.887s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 22\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.08558839408769495\n",
      "Autoencoder Train loss: 0.055638772169394154\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.03498400664991803\n",
      "Autoencoder Test loss: 0.05117751854575343\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 7.931s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 23\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.08510016401608785\n",
      "Autoencoder Train loss: 0.055057037799131306\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.03525729477405548\n",
      "Autoencoder Test loss: 0.05041978073616823\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 8.110s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 24\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.0844786527256171\n",
      "Autoencoder Train loss: 0.05454898190995058\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.035250743540624775\n",
      "Autoencoder Test loss: 0.04997847808731927\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 7.944s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 25\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.08384316955648717\n",
      "Autoencoder Train loss: 0.054280756041407585\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.03478828062199884\n",
      "Autoencoder Test loss: 0.04999344454457363\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 8.225s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 26\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.08342123501712367\n",
      "Autoencoder Train loss: 0.05442039704038983\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.03371738952895006\n",
      "Autoencoder Test loss: 0.049515164664222136\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 8.833s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 27\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.0828303276073365\n",
      "Autoencoder Train loss: 0.05385918680223681\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.03401983963946501\n",
      "Autoencoder Test loss: 0.049654334887034364\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 8.656s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 28\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.08247024663502261\n",
      "Autoencoder Train loss: 0.05365932018806537\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.03339966810825798\n",
      "Autoencoder Test loss: 0.049326069859994784\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 8.814s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 29\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.08206199322428022\n",
      "Autoencoder Train loss: 0.05318775059034427\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.03307211450818512\n",
      "Autoencoder Test loss: 0.04923735838383436\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 8.630s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 30\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.08174015625956513\n",
      "Autoencoder Train loss: 0.05279190062234799\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.033372055635684066\n",
      "Autoencoder Test loss: 0.049087058235373765\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 8.727s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 31\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.08136652099589507\n",
      "Autoencoder Train loss: 0.05232869727270944\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.033836039093633495\n",
      "Autoencoder Test loss: 0.04785539996292856\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 8.103s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 32\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.08081058944974627\n",
      "Autoencoder Train loss: 0.0516660250723362\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.032709672995325595\n",
      "Autoencoder Test loss: 0.04781890194863081\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 7.903s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 33\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.08062503105472951\n",
      "Autoencoder Train loss: 0.05128642863460949\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.03279077261686325\n",
      "Autoencoder Test loss: 0.047598393427001104\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 7.990s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 34\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.08019602476131349\n",
      "Autoencoder Train loss: 0.05086983327886888\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.03247012058272958\n",
      "Autoencoder Test loss: 0.04720490508609348\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 8.248s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 35\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.08003548027149268\n",
      "Autoencoder Train loss: 0.05076784387763057\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.031495574240883194\n",
      "Autoencoder Test loss: 0.047188334063523345\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.001]\n",
      "Epoch time: epoch_time = 8.550s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 36\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.07975738690722556\n",
      "Autoencoder Train loss: 0.05059646988021476\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.03262208733293745\n",
      "Autoencoder Test loss: 0.047405091010861926\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.0001]\n",
      "Epoch time: epoch_time = 8.212s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 37\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.07958265633455344\n",
      "Autoencoder Train loss: 0.05049977158861501\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.03232682806750139\n",
      "Autoencoder Test loss: 0.046941661896804966\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.0001]\n",
      "Epoch time: epoch_time = 8.350s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 38\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.07915738952301797\n",
      "Autoencoder Train loss: 0.05051810210127206\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.03253172720885939\n",
      "Autoencoder Test loss: 0.047421766341560416\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.0001]\n",
      "Epoch time: epoch_time = 8.411s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 39\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.07895884574169204\n",
      "Autoencoder Train loss: 0.0505740111250253\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.03151134872395131\n",
      "Autoencoder Test loss: 0.047114185264541045\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.0001]\n",
      "Epoch time: epoch_time = 8.497s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 40\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.07842354343405791\n",
      "Autoencoder Train loss: 0.05070174361268679\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.03260015851507584\n",
      "Autoencoder Test loss: 0.04745131037715408\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.0001]\n",
      "Epoch time: epoch_time = 8.424s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 41\n",
      "---------\n",
      "--------------------------------------\n",
      "Forcasting Train loss: 0.07839584971467654\n",
      "Autoencoder Train loss: 0.050877729386446024\n",
      "--------------------------------------\n",
      "Forcasting Test loss: 0.03213250000650684\n",
      "Autoencoder Test loss: 0.0474225879750318\n",
      "--------------------------------------\n",
      "Forecasting lr: [0.001]\n",
      "Autoencoder lr: [0.0001]\n",
      "Epoch time: epoch_time = 8.121s\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Epoch 42\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "trainer.train(n_epochs=200)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(model, model_path)\n",
    "torch.cuda.empty_cache()\n",
    "del model"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
