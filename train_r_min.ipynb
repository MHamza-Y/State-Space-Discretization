{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from base_rl.eval_policy import EvalDiscreteStatePolicy\n",
    "from dynamic_programming.mdp_model import MDPModel\n",
    "from dynamic_programming.policy import DPPolicy\n",
    "from envs.env_creator import env_creator\n",
    "from envs.plot import plot_industrial_benchmark_trajectories\n",
    "from rmin.train import RMinTrainer\n",
    "from experiments.offline_experiment_configs import RMinExperimentConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [20, 12]\n",
    "fixed_digits = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "model_names = ['model_aeq-20bits3']\n",
    "root_path = 'tmp'\n",
    "training_episodes = [10, 100, 1000, 10000]\n",
    "min_count = [1, 2, 3, 5]\n",
    "total_epochs=500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and MDP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(training_episodes) != len(min_count):\n",
    "    raise Exception('training_episodes and min_count must be of same length')\n",
    "\n",
    "trajectory_paths = [os.path.join(root_path, \"offline_rl_trajectories\", model, \"rl_dataset.npy\") for model in model_names]\n",
    "steps_per_episode = 1000\n",
    "\n",
    "\n",
    "experiment_configs = []\n",
    "device = 'cpu'\n",
    "for model_name in model_names:\n",
    "    for i, training_episode in enumerate(training_episodes):\n",
    "        experiment_configs.append(\n",
    "            RMinExperimentConfig(\n",
    "                model_name=model_name,\n",
    "                model_path=os.path.join(root_path, 'state_quantization', model_name),\n",
    "                dataset_path=os.path.join(root_path, \"offline_rl_trajectories\", model_name, \"rl_dataset.npy\"),\n",
    "                mdp_path=os.path.join(root_path, 'rmin', 'mdp', model_name, f'{training_episode}', 'mdp_model.pkl'),\n",
    "                policy_path=os.path.join(root_path, 'rmin', model_name, f'{training_episode}',\n",
    "                                         'policy.pkl'),\n",
    "                dataset_size=training_episode * steps_per_episode,\n",
    "                r_min=min_count[i]\n",
    "            )\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create MDP Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynamic_programming.mdp_model import create_mdp_models\n",
    "\n",
    "for config in experiment_configs:\n",
    "    create_mdp_models(load_path=config.dataset_path, mdp_save_path=config.mdp_path, reward_function_type='state_action',\n",
    "                      device=device, dataset_size=config.dataset_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_r_min(mdp_path, policy_save_path):\n",
    "    mdp_model = MDPModel.load(mdp_path)\n",
    "    solver = RMinTrainer(reward_function=mdp_model.reward_function, transition_model=mdp_model.transition_model,\n",
    "                         count_state_action=mdp_model.count_state_action, min_count=min_count[i])\n",
    "    solver.train(epochs=total_epochs, gamma=0.995)\n",
    "    trained_policy = DPPolicy(policy_table=solver.get_policy(), state_to_index=mdp_model.state_to_index,\n",
    "                              index_to_action=mdp_model.index_to_actions)\n",
    "    trained_policy.save(policy_save_path)\n",
    "\n",
    "\n",
    "for config in experiment_configs:\n",
    "    print(config.mdp_path)\n",
    "    train_r_min(mdp_path=config.mdp_path, policy_save_path=config.policy_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmarks.policy_benchmarks import PolicyBenchmarks\n",
    "\n",
    "steps_per_episode = 1000\n",
    "evaluators = []\n",
    "for config in experiment_configs:\n",
    "    print(config.__dict__)\n",
    "    eval_policy = DPPolicy.load(config.policy_path)\n",
    "    env_kwargs = {'steps_per_episode': steps_per_episode, 'device': device, 'model_path': config.model_path}\n",
    "    evaluator = EvalDiscreteStatePolicy(policy=eval_policy, env_creator=env_creator, env_kwargs=env_kwargs,\n",
    "                                        tag=f'{config.model_name}/{config.dataset_size}')\n",
    "    evaluators.append(evaluator)\n",
    "\n",
    "policy_benchmarks = PolicyBenchmarks(evaluators=evaluators, epochs=10)\n",
    "policy_benchmarks.benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_industrial_benchmark_trajectories(policy_benchmarks.evaluators[-2].eval_trajectories[0]['info'])\n",
    "np.mean(policy_benchmarks.evaluators[-2].eval_rewards_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(policy_benchmarks.benchmark_metrics)\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [20, 12]\n",
    "\n",
    "\n",
    "for model_name in model_names:\n",
    "    m = np.core.defchararray.find(df.columns.values.astype(str), model_name) >= 0\n",
    "    fdf = df.loc[:, m]\n",
    "    ax = fdf.plot.bar()\n",
    "\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
