{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import makedirs\n",
    "from os.path import join, dirname\n",
    "import uuid\n",
    "import torch\n",
    "from functools import partial\n",
    "from papermill import execute_notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initial Setup\n",
    "\n",
    "Create directory structure and initialize parameters for the experiment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment ID: 302e1c19-b79a-46a8-bc5f-ad3227eabd00\n"
     ]
    }
   ],
   "source": [
    "experiment_id = uuid.uuid4().__str__()\n",
    "print(f'Experiment ID: {experiment_id}')\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "root_path = join('tmp', 'experiments', experiment_id)\n",
    "notebook_out_path = join(root_path, 'notebook_outputs')\n",
    "tensorboard_dir = join(root_path, 'tensorboard_logs')\n",
    "forecasting_models_root_save_path = join(root_path, 'state_quantization')\n",
    "makedirs(dirname(root_path), exist_ok=True)\n",
    "makedirs(notebook_out_path, exist_ok=True)\n",
    "makedirs(dirname(tensorboard_dir), exist_ok=True)\n",
    "\n",
    "bits = 20\n",
    "model_names = [f'model_h_c-{bits}bits', f'model_aeq-{bits}bits', f'model_final_h-{bits}bits']\n",
    "log_output = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Forecasting Model Training Tasks\n",
    "\n",
    "These tasks execute the notebooks which are responsible for the training of forecasting models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "forecasting_tensorboard_dir = join(tensorboard_dir,'forecasting_models')\n",
    "makedirs(dirname(forecasting_tensorboard_dir), exist_ok=True)\n",
    "train_lstm_ae_notebook = 'train-LSTM-AE.ipynb'\n",
    "\n",
    "train_lstm_ae_task = partial(execute_notebook,\n",
    "\n",
    "                             input_path=train_lstm_ae_notebook,\n",
    "                             output_path=join(notebook_out_path, train_lstm_ae_notebook),\n",
    "                             parameters=\n",
    "                             dict(\n",
    "                                 bits=bits,\n",
    "                                 use_cuda=use_cuda,\n",
    "                                 model_path=join(forecasting_models_root_save_path, f'model_aeq-{bits}bits'),\n",
    "                                 untrained_model_path=join(forecasting_models_root_save_path,\n",
    "                                                           f'untrained_model_aeq-{bits}bits'),\n",
    "                                 log_dir=forecasting_tensorboard_dir\n",
    "                             ),\n",
    "                             log_output=log_output\n",
    "                             )\n",
    "\n",
    "train_discHC_notebook = 'train_DiscHC.ipynb'\n",
    "train_discHC_task = partial(execute_notebook,\n",
    "                            input_path=train_discHC_notebook,\n",
    "                            output_path=join(notebook_out_path, train_discHC_notebook),\n",
    "                            parameters=\n",
    "                            dict(\n",
    "                                bits=bits,\n",
    "                                use_cuda=use_cuda,\n",
    "                                model_path=join(forecasting_models_root_save_path, f'model_h_c-{bits}bits'),\n",
    "                                untrained_model_path=join(forecasting_models_root_save_path,\n",
    "                                                          f'untrained_model_h_c-{bits}bits'),\n",
    "                                model_dict_path=join(forecasting_models_root_save_path, 'model_h_c_dict'),\n",
    "                                log_dir=forecasting_tensorboard_dir\n",
    "                            ),\n",
    "                            log_output=log_output)\n",
    "\n",
    "train_disc_final_h_notebook = 'train_DiscFinalH.ipynb'\n",
    "train_disc_final_h_task = partial(execute_notebook,\n",
    "                                  input_path=train_disc_final_h_notebook,\n",
    "                                  output_path=join(notebook_out_path,\n",
    "                                                   train_disc_final_h_notebook),\n",
    "                                  parameters=\n",
    "                                  dict(\n",
    "                                      bits=bits,\n",
    "                                      use_cuda=use_cuda,\n",
    "                                      model_path=join(forecasting_models_root_save_path, f'model_final_h-{bits}bits'),\n",
    "                                      untrained_model_path=join(forecasting_models_root_save_path,\n",
    "                                                                f'untrained_model_final_h-{bits}bits'),\n",
    "                                      model_dict_path=join(forecasting_models_root_save_path, 'model_final_h_dict'),\n",
    "                                      log_dir=forecasting_tensorboard_dir\n",
    "                                  ),\n",
    "                                  log_output=log_output\n",
    "                                  )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Model Based Q Learning Tasks\n",
    "\n",
    "These tasks run the model based q learning notebooks for each trained and untrained forecasting model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "mb_q_learning_tensorboard_dir = join(tensorboard_dir,'mb_q_learning')\n",
    "makedirs(dirname(mb_q_learning_tensorboard_dir), exist_ok=True)\n",
    "train_model_based_q_learning_tasks = []\n",
    "train_model_based_q_learning_task_notebook = 'train_mb_offline_q_learning.ipynb'\n",
    "q_learning_epochs = 10\n",
    "\n",
    "for model_name in model_names:\n",
    "    train_model_based_q_learning_tasks.append(partial(\n",
    "        execute_notebook,\n",
    "        input_path=train_model_based_q_learning_task_notebook,\n",
    "        output_path=join(notebook_out_path, f'{model_name}_{train_model_based_q_learning_task_notebook}'),\n",
    "        parameters=\n",
    "        dict(\n",
    "            device='cuda' if use_cuda else 'cpu',\n",
    "            root_path=root_path,\n",
    "            total_epochs=q_learning_epochs,\n",
    "            model_name=model_name,\n",
    "            log_dir=mb_q_learning_tensorboard_dir\n",
    "        ),\n",
    "        log_output=log_output\n",
    "    ))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Q Learning Tasks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "q_learning_tensorboard_dir = join(tensorboard_dir,'q_learning')\n",
    "makedirs(dirname(q_learning_tensorboard_dir), exist_ok=True)\n",
    "train_q_learning_tasks = []\n",
    "train_q_learning_task_notebook = 'train_q_learning.ipynb'\n",
    "\n",
    "for model_name in model_names:\n",
    "    train_q_learning_tasks.append(partial(\n",
    "        execute_notebook,\n",
    "        input_path=train_q_learning_task_notebook,\n",
    "        output_path=join(notebook_out_path, f'{model_name}_{train_q_learning_task_notebook}'),\n",
    "        parameters=\n",
    "        dict(\n",
    "            device='cuda' if use_cuda else 'cpu',\n",
    "            root_path=root_path,\n",
    "            total_epochs=q_learning_epochs,\n",
    "            model_name=model_name,\n",
    "            log_dir=q_learning_tensorboard_dir\n",
    "        ),\n",
    "        log_output=log_output\n",
    "    ))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Offline Dataset Creation Task"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Q Learning Tasks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-19 14:25:15.508478: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-19 14:25:15.447975: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-19 14:25:15.481127: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-19 14:25:15.447975: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "2022-12-19 14:25:15.481127: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "2022-12-19 14:25:15.508478: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "2022-12-19 14:25:17.754743: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:25:17.755042: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:25:17.755049: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-12-19 14:25:17.754738: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:25:17.754743: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:25:17.755042: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:25:17.755049: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-12-19 14:25:17.755040: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:25:17.755048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-12-19 14:25:17.754743: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:25:17.755042: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:25:17.755049: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "\n",
      "2022-12-19 14:25:17.754743: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:25:17.755040: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:25:17.755048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "\n",
      "2022-12-19 14:25:17.754738: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:25:17.755042: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:25:17.755049: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "\n",
      "/home/hamza/PycharmProjects/State-Space-Discretization/venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "\n",
      "Input notebook does not contain a cell with tag 'parameters'\n"
     ]
    },
    {
     "data": {
      "text/plain": "Executing:   0%|          | 0/7 [00:00<?, ?cell/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "82a0bf7f80604758a2b58cb20a7d4256"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input notebook does not contain a cell with tag 'parameters'\n",
      "Input notebook does not contain a cell with tag 'parameters'\n"
     ]
    },
    {
     "data": {
      "text/plain": "Executing:   0%|          | 0/7 [00:00<?, ?cell/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "46517901b4cc48218635ac148f654676"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Executing:   0%|          | 0/7 [00:00<?, ?cell/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a98221c0e716423b9f1ce7e5ad224ffd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-19 14:27:31.405590: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-19 14:27:31.417582: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-19 14:27:31.447850: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-19 14:27:31.435155: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-19 14:27:31.475818: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-19 14:27:31.472500: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-19 14:27:31.405590: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "2022-12-19 14:27:31.447850: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "2022-12-19 14:27:31.435155: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "2022-12-19 14:27:31.417582: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "2022-12-19 14:27:31.475818: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "2022-12-19 14:27:31.472500: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "2022-12-19 14:27:32.234109: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:27:32.234100: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:27:32.234100: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:27:32.234100: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:27:32.234100: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:27:32.234185: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:27:32.234193: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-12-19 14:27:32.234185: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:27:32.234193: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-12-19 14:27:32.234187: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:27:32.234193: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-12-19 14:27:32.234184: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:27:32.234192: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-12-19 14:27:32.234184: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:27:32.234195: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-12-19 14:27:32.234100: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:27:32.234185: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:27:32.234193: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-12-19 14:27:32.234100: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:27:32.234185: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:27:32.234193: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "\n",
      "2022-12-19 14:27:32.234100: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:27:32.234185: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:27:32.234193: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "\n",
      "2022-12-19 14:27:32.234109: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:27:32.234184: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:27:32.234195: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "\n",
      "2022-12-19 14:27:32.234100: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:27:32.234185: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:27:32.234193: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "\n",
      "2022-12-19 14:27:32.234100: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:27:32.234187: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:27:32.234193: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "\n",
      "2022-12-19 14:27:32.234100: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:27:32.234184: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 14:27:32.234192: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "\n",
      "/home/hamza/PycharmProjects/State-Space-Discretization/venv/lib/python3.10/site-packages/gym/spaces/box.py:73: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "  logger.warn(\n",
      "\n",
      "/home/hamza/PycharmProjects/State-Space-Discretization/venv/lib/python3.10/site-packages/gym/spaces/box.py:73: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "  logger.warn(\n",
      "\n",
      "/home/hamza/PycharmProjects/State-Space-Discretization/venv/lib/python3.10/site-packages/gym/spaces/box.py:73: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "  logger.warn(\n",
      "\n",
      "/home/hamza/PycharmProjects/State-Space-Discretization/venv/lib/python3.10/site-packages/gym/spaces/box.py:73: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "  logger.warn(\n",
      "\n",
      "/home/hamza/PycharmProjects/State-Space-Discretization/venv/lib/python3.10/site-packages/gym/spaces/box.py:73: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "  logger.warn(\n",
      "\n",
      "/home/hamza/PycharmProjects/State-Space-Discretization/venv/lib/python3.10/site-packages/gym/spaces/box.py:73: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "  logger.warn(\n",
      "\n"
     ]
    },
    {
     "ename": "PapermillExecutionError",
     "evalue": "\n---------------------------------------------------------------------------\nException encountered at \"In [7]\":\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[7], line 13\n      7 callbacks = [\n      8     SaveBestPolicy(save_path=best_save_path),\n      9     SavePolicyOnTrainingEnd(save_path=last_save_path),\n     10     SavePolicyXInterval(x_interval_save_path)\n     11 ]\n     12 trainer = OnlineTrainer(algo=algo,callbacks=callbacks)\n---> 13 trainer.fit()\n\nFile ~/PycharmProjects/State-Space-Discretization/base_rl/train.py:28, in OnlineTrainer.fit(self)\n     26 def fit(self):\n     27     self.execute_callback(self.on_training_start_cb)\n---> 28     self.algo.setup()\n     29     while self.algo.keep_training():\n     30         self.execute_callback(self.on_episode_start_cb)\n\nFile ~/PycharmProjects/State-Space-Discretization/q_learning/algorithm.py:69, in QLearningAlgo.setup(self)\n     67 def setup(self):\n     68     self.writer = SummaryWriter(comment=self.comment, log_dir=self.log_dir)\n---> 69     self.env = self.env_creator(**self.env_kwargs)\n     70     self.mean_train_reward_per_epoch = []\n     71     self.current_epoch = 0\n\nFile ~/PycharmProjects/State-Space-Discretization/envs/env_creator.py:29, in IBGymModelQ_creator(model_path, device, steps_per_episode)\n     27 normalize_output = NormalizeTransform.load('tmp/transformer/NormalizeOutputConfigs.pkl').to(device)\n     28 lstm_quantize = LSTMQuantize(model=model, normalize_transformer=normalize_input, reshape=reshape)\n---> 29 return IBGymModelQ(device=device, setpoint=70, reward_type='classic', action_type='discrete',\n     30                    observation_type='include_past',\n     31                    reset_after_timesteps=steps_per_episode, n_past_timesteps=model.get_seq_len(),\n     32                    lstm_quantize=lstm_quantize, output_normalize_transform=normalize_output)\n\nFile ~/PycharmProjects/State-Space-Discretization/envs/IBGym_mod_envs.py:271, in IBGymModelQ.__init__(self, lstm_quantize, output_normalize_transform, device, **kwargs)\n    269 self.h = 0\n    270 self.output_normalize_transform = output_normalize_transform\n--> 271 super().__init__(**kwargs)\n\nFile ~/PycharmProjects/State-Space-Discretization/envs/IBGym_mod_envs.py:90, in IBGymModded.__init__(self, setpoint, reward_type, action_type, observation_type, reset_after_timesteps, init_seed, n_past_timesteps)\n     87 else:\n     88     raise ValueError('Invalid observation_type. observation_type can either be \"classic\" or \"include_past\"')\n---> 90 self.reset()\n\nFile ~/PycharmProjects/State-Space-Discretization/envs/IBGym_mod_envs.py:308, in IBGymModelQ.reset(self)\n    306 self.done = False\n    307 self.last_observation = return_observation\n--> 308 discrete_obs = self.lstm_quantize(return_observation)[0]\n    309 self.model_out = self.lstm_quantize.get_continuous_output()\n    310 self.v = self.last_observation[1]\n\nFile ~/PycharmProjects/State-Space-Discretization/state_quantization/transforms.py:76, in LSTMQuantize.__call__(self, x)\n     74 x = self.normalize_transformer.transform(x)\n     75 x = torch.nan_to_num(x, 1)\n---> 76 self.y = self.model(x)\n     77 return self.bin2dec(self.model.quantized_state).tolist()\n\nFile ~/PycharmProjects/State-Space-Discretization/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190, in Module._call_impl(self, *input, **kwargs)\n   1186 # If we don't have any hooks, we want to skip the rest of the logic in\n   1187 # this function, and just call forward.\n   1188 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1189         or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1190     return forward_call(*input, **kwargs)\n   1191 # Do not call functions when jit is used\n   1192 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/PycharmProjects/State-Space-Discretization/state_quantization/quantization_models.py:79, in ForcastingDiscHC.forward(self, x)\n     76 (h, c) = self.init_hidden(x.shape[0])\n     78 for i in range(self.seq_len):\n---> 79     self.lstm_layers_forward(x=x[:, i, :], h=h, c=c)\n     81 self.quantized_state = torch.cat((h[-1], c[-1]), dim=1)\n     83 output = self.final_dense_forward(h[-1])\n\nFile ~/PycharmProjects/State-Space-Discretization/state_quantization/quantization_models.py:67, in ForcastingDiscHC.lstm_layers_forward(self, x, h, c)\n     65 layer_input = x\n     66 for layer_idx in range(self.n_layers):\n---> 67     (h[layer_idx], c[layer_idx]) = self.lstm_layers[layer_idx](layer_input, (h[layer_idx], c[layer_idx]))\n     68     h[layer_idx] = self.h_quantization_layers[layer_idx](h[layer_idx])\n     69     c[layer_idx] = self.c_quantization_layers[layer_idx](c[layer_idx])\n\nFile ~/PycharmProjects/State-Space-Discretization/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190, in Module._call_impl(self, *input, **kwargs)\n   1186 # If we don't have any hooks, we want to skip the rest of the logic in\n   1187 # this function, and just call forward.\n   1188 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1189         or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1190     return forward_call(*input, **kwargs)\n   1191 # Do not call functions when jit is used\n   1192 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/PycharmProjects/State-Space-Discretization/venv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1194, in LSTMCell.forward(self, input, hx)\n   1191 else:\n   1192     hx = (hx[0].unsqueeze(0), hx[1].unsqueeze(0)) if not is_batched else hx\n-> 1194 ret = _VF.lstm_cell(\n   1195     input, hx,\n   1196     self.weight_ih, self.weight_hh,\n   1197     self.bias_ih, self.bias_hh,\n   1198 )\n   1200 if not is_batched:\n   1201     ret = (ret[0].squeeze(0), ret[1].squeeze(0))\n\nRuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRemoteTraceback\u001B[0m                           Traceback (most recent call last)",
      "\u001B[0;31mRemoteTraceback\u001B[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/hamza/PycharmProjects/State-Space-Discretization/venv/lib/python3.10/site-packages/papermill/execute.py\", line 128, in execute_notebook\n    raise_for_execution_errors(nb, output_path)\n  File \"/home/hamza/PycharmProjects/State-Space-Discretization/venv/lib/python3.10/site-packages/papermill/execute.py\", line 232, in raise_for_execution_errors\n    raise error\npapermill.exceptions.PapermillExecutionError: \n---------------------------------------------------------------------------\nException encountered at \"In [7]\":\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[7], line 13\n      7 callbacks = [\n      8     SaveBestPolicy(save_path=best_save_path),\n      9     SavePolicyOnTrainingEnd(save_path=last_save_path),\n     10     SavePolicyXInterval(x_interval_save_path)\n     11 ]\n     12 trainer = OnlineTrainer(algo=algo,callbacks=callbacks)\n---> 13 trainer.fit()\n\nFile ~/PycharmProjects/State-Space-Discretization/base_rl/train.py:28, in OnlineTrainer.fit(self)\n     26 def fit(self):\n     27     self.execute_callback(self.on_training_start_cb)\n---> 28     self.algo.setup()\n     29     while self.algo.keep_training():\n     30         self.execute_callback(self.on_episode_start_cb)\n\nFile ~/PycharmProjects/State-Space-Discretization/q_learning/algorithm.py:69, in QLearningAlgo.setup(self)\n     67 def setup(self):\n     68     self.writer = SummaryWriter(comment=self.comment, log_dir=self.log_dir)\n---> 69     self.env = self.env_creator(**self.env_kwargs)\n     70     self.mean_train_reward_per_epoch = []\n     71     self.current_epoch = 0\n\nFile ~/PycharmProjects/State-Space-Discretization/envs/env_creator.py:29, in IBGymModelQ_creator(model_path, device, steps_per_episode)\n     27 normalize_output = NormalizeTransform.load('tmp/transformer/NormalizeOutputConfigs.pkl').to(device)\n     28 lstm_quantize = LSTMQuantize(model=model, normalize_transformer=normalize_input, reshape=reshape)\n---> 29 return IBGymModelQ(device=device, setpoint=70, reward_type='classic', action_type='discrete',\n     30                    observation_type='include_past',\n     31                    reset_after_timesteps=steps_per_episode, n_past_timesteps=model.get_seq_len(),\n     32                    lstm_quantize=lstm_quantize, output_normalize_transform=normalize_output)\n\nFile ~/PycharmProjects/State-Space-Discretization/envs/IBGym_mod_envs.py:271, in IBGymModelQ.__init__(self, lstm_quantize, output_normalize_transform, device, **kwargs)\n    269 self.h = 0\n    270 self.output_normalize_transform = output_normalize_transform\n--> 271 super().__init__(**kwargs)\n\nFile ~/PycharmProjects/State-Space-Discretization/envs/IBGym_mod_envs.py:90, in IBGymModded.__init__(self, setpoint, reward_type, action_type, observation_type, reset_after_timesteps, init_seed, n_past_timesteps)\n     87 else:\n     88     raise ValueError('Invalid observation_type. observation_type can either be \"classic\" or \"include_past\"')\n---> 90 self.reset()\n\nFile ~/PycharmProjects/State-Space-Discretization/envs/IBGym_mod_envs.py:308, in IBGymModelQ.reset(self)\n    306 self.done = False\n    307 self.last_observation = return_observation\n--> 308 discrete_obs = self.lstm_quantize(return_observation)[0]\n    309 self.model_out = self.lstm_quantize.get_continuous_output()\n    310 self.v = self.last_observation[1]\n\nFile ~/PycharmProjects/State-Space-Discretization/state_quantization/transforms.py:76, in LSTMQuantize.__call__(self, x)\n     74 x = self.normalize_transformer.transform(x)\n     75 x = torch.nan_to_num(x, 1)\n---> 76 self.y = self.model(x)\n     77 return self.bin2dec(self.model.quantized_state).tolist()\n\nFile ~/PycharmProjects/State-Space-Discretization/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190, in Module._call_impl(self, *input, **kwargs)\n   1186 # If we don't have any hooks, we want to skip the rest of the logic in\n   1187 # this function, and just call forward.\n   1188 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1189         or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1190     return forward_call(*input, **kwargs)\n   1191 # Do not call functions when jit is used\n   1192 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/PycharmProjects/State-Space-Discretization/state_quantization/quantization_models.py:79, in ForcastingDiscHC.forward(self, x)\n     76 (h, c) = self.init_hidden(x.shape[0])\n     78 for i in range(self.seq_len):\n---> 79     self.lstm_layers_forward(x=x[:, i, :], h=h, c=c)\n     81 self.quantized_state = torch.cat((h[-1], c[-1]), dim=1)\n     83 output = self.final_dense_forward(h[-1])\n\nFile ~/PycharmProjects/State-Space-Discretization/state_quantization/quantization_models.py:67, in ForcastingDiscHC.lstm_layers_forward(self, x, h, c)\n     65 layer_input = x\n     66 for layer_idx in range(self.n_layers):\n---> 67     (h[layer_idx], c[layer_idx]) = self.lstm_layers[layer_idx](layer_input, (h[layer_idx], c[layer_idx]))\n     68     h[layer_idx] = self.h_quantization_layers[layer_idx](h[layer_idx])\n     69     c[layer_idx] = self.c_quantization_layers[layer_idx](c[layer_idx])\n\nFile ~/PycharmProjects/State-Space-Discretization/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190, in Module._call_impl(self, *input, **kwargs)\n   1186 # If we don't have any hooks, we want to skip the rest of the logic in\n   1187 # this function, and just call forward.\n   1188 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1189         or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1190     return forward_call(*input, **kwargs)\n   1191 # Do not call functions when jit is used\n   1192 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/PycharmProjects/State-Space-Discretization/venv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1194, in LSTMCell.forward(self, input, hx)\n   1191 else:\n   1192     hx = (hx[0].unsqueeze(0), hx[1].unsqueeze(0)) if not is_batched else hx\n-> 1194 ret = _VF.lstm_cell(\n   1195     input, hx,\n   1196     self.weight_ih, self.weight_hh,\n   1197     self.bias_ih, self.bias_hh,\n   1198 )\n   1200 if not is_batched:\n   1201     ret = (ret[0].squeeze(0), ret[1].squeeze(0))\n\nRuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`\n\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mPapermillExecutionError\u001B[0m                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 23\u001B[0m\n\u001B[1;32m     20\u001B[0m     q_results\u001B[38;5;241m.\u001B[39mappend(result)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m mb_q_result \u001B[38;5;129;01min\u001B[39;00m mb_q_results:\n\u001B[0;32m---> 23\u001B[0m     \u001B[43mmb_q_result\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m q_result \u001B[38;5;129;01min\u001B[39;00m q_results:\n\u001B[1;32m     26\u001B[0m     q_result\u001B[38;5;241m.\u001B[39mget()\n",
      "File \u001B[0;32m/usr/lib/python3.10/multiprocessing/pool.py:774\u001B[0m, in \u001B[0;36mApplyResult.get\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    772\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_value\n\u001B[1;32m    773\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 774\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_value\n",
      "\u001B[0;31mPapermillExecutionError\u001B[0m: \n---------------------------------------------------------------------------\nException encountered at \"In [7]\":\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[7], line 13\n      7 callbacks = [\n      8     SaveBestPolicy(save_path=best_save_path),\n      9     SavePolicyOnTrainingEnd(save_path=last_save_path),\n     10     SavePolicyXInterval(x_interval_save_path)\n     11 ]\n     12 trainer = OnlineTrainer(algo=algo,callbacks=callbacks)\n---> 13 trainer.fit()\n\nFile ~/PycharmProjects/State-Space-Discretization/base_rl/train.py:28, in OnlineTrainer.fit(self)\n     26 def fit(self):\n     27     self.execute_callback(self.on_training_start_cb)\n---> 28     self.algo.setup()\n     29     while self.algo.keep_training():\n     30         self.execute_callback(self.on_episode_start_cb)\n\nFile ~/PycharmProjects/State-Space-Discretization/q_learning/algorithm.py:69, in QLearningAlgo.setup(self)\n     67 def setup(self):\n     68     self.writer = SummaryWriter(comment=self.comment, log_dir=self.log_dir)\n---> 69     self.env = self.env_creator(**self.env_kwargs)\n     70     self.mean_train_reward_per_epoch = []\n     71     self.current_epoch = 0\n\nFile ~/PycharmProjects/State-Space-Discretization/envs/env_creator.py:29, in IBGymModelQ_creator(model_path, device, steps_per_episode)\n     27 normalize_output = NormalizeTransform.load('tmp/transformer/NormalizeOutputConfigs.pkl').to(device)\n     28 lstm_quantize = LSTMQuantize(model=model, normalize_transformer=normalize_input, reshape=reshape)\n---> 29 return IBGymModelQ(device=device, setpoint=70, reward_type='classic', action_type='discrete',\n     30                    observation_type='include_past',\n     31                    reset_after_timesteps=steps_per_episode, n_past_timesteps=model.get_seq_len(),\n     32                    lstm_quantize=lstm_quantize, output_normalize_transform=normalize_output)\n\nFile ~/PycharmProjects/State-Space-Discretization/envs/IBGym_mod_envs.py:271, in IBGymModelQ.__init__(self, lstm_quantize, output_normalize_transform, device, **kwargs)\n    269 self.h = 0\n    270 self.output_normalize_transform = output_normalize_transform\n--> 271 super().__init__(**kwargs)\n\nFile ~/PycharmProjects/State-Space-Discretization/envs/IBGym_mod_envs.py:90, in IBGymModded.__init__(self, setpoint, reward_type, action_type, observation_type, reset_after_timesteps, init_seed, n_past_timesteps)\n     87 else:\n     88     raise ValueError('Invalid observation_type. observation_type can either be \"classic\" or \"include_past\"')\n---> 90 self.reset()\n\nFile ~/PycharmProjects/State-Space-Discretization/envs/IBGym_mod_envs.py:308, in IBGymModelQ.reset(self)\n    306 self.done = False\n    307 self.last_observation = return_observation\n--> 308 discrete_obs = self.lstm_quantize(return_observation)[0]\n    309 self.model_out = self.lstm_quantize.get_continuous_output()\n    310 self.v = self.last_observation[1]\n\nFile ~/PycharmProjects/State-Space-Discretization/state_quantization/transforms.py:76, in LSTMQuantize.__call__(self, x)\n     74 x = self.normalize_transformer.transform(x)\n     75 x = torch.nan_to_num(x, 1)\n---> 76 self.y = self.model(x)\n     77 return self.bin2dec(self.model.quantized_state).tolist()\n\nFile ~/PycharmProjects/State-Space-Discretization/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190, in Module._call_impl(self, *input, **kwargs)\n   1186 # If we don't have any hooks, we want to skip the rest of the logic in\n   1187 # this function, and just call forward.\n   1188 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1189         or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1190     return forward_call(*input, **kwargs)\n   1191 # Do not call functions when jit is used\n   1192 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/PycharmProjects/State-Space-Discretization/state_quantization/quantization_models.py:79, in ForcastingDiscHC.forward(self, x)\n     76 (h, c) = self.init_hidden(x.shape[0])\n     78 for i in range(self.seq_len):\n---> 79     self.lstm_layers_forward(x=x[:, i, :], h=h, c=c)\n     81 self.quantized_state = torch.cat((h[-1], c[-1]), dim=1)\n     83 output = self.final_dense_forward(h[-1])\n\nFile ~/PycharmProjects/State-Space-Discretization/state_quantization/quantization_models.py:67, in ForcastingDiscHC.lstm_layers_forward(self, x, h, c)\n     65 layer_input = x\n     66 for layer_idx in range(self.n_layers):\n---> 67     (h[layer_idx], c[layer_idx]) = self.lstm_layers[layer_idx](layer_input, (h[layer_idx], c[layer_idx]))\n     68     h[layer_idx] = self.h_quantization_layers[layer_idx](h[layer_idx])\n     69     c[layer_idx] = self.c_quantization_layers[layer_idx](c[layer_idx])\n\nFile ~/PycharmProjects/State-Space-Discretization/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190, in Module._call_impl(self, *input, **kwargs)\n   1186 # If we don't have any hooks, we want to skip the rest of the logic in\n   1187 # this function, and just call forward.\n   1188 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1189         or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1190     return forward_call(*input, **kwargs)\n   1191 # Do not call functions when jit is used\n   1192 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/PycharmProjects/State-Space-Discretization/venv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1194, in LSTMCell.forward(self, input, hx)\n   1191 else:\n   1192     hx = (hx[0].unsqueeze(0), hx[1].unsqueeze(0)) if not is_batched else hx\n-> 1194 ret = _VF.lstm_cell(\n   1195     input, hx,\n   1196     self.weight_ih, self.weight_hh,\n   1197     self.bias_ih, self.bias_hh,\n   1198 )\n   1200 if not is_batched:\n   1201     ret = (ret[0].squeeze(0), ret[1].squeeze(0))\n\nRuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "from multiprocessing.pool import Pool\n",
    "start = time.time()\n",
    "with Pool(3) as pool:\n",
    "    train_lstm_ae_task_result = pool.apply_async(train_lstm_ae_task)\n",
    "    train_discHC_task_result = pool.apply_async(train_discHC_task)\n",
    "    train_disc_final_h_task_result = pool.apply_async(train_disc_final_h_task)\n",
    "\n",
    "    train_lstm_ae_task_result.get()\n",
    "    train_discHC_task_result.get()\n",
    "    train_disc_final_h_task_result.get()\n",
    "\n",
    "    mb_q_results = []\n",
    "    for train_model_based_q_learning_task in train_model_based_q_learning_tasks:\n",
    "        result = pool.apply_async(train_model_based_q_learning_task)\n",
    "        mb_q_results.append(result)\n",
    "\n",
    "    q_results = []\n",
    "    for train_q_learning_task in train_q_learning_tasks:\n",
    "        result = pool.apply_async(train_q_learning_task)\n",
    "        q_results.append(result)\n",
    "\n",
    "    for mb_q_result in mb_q_results:\n",
    "        mb_q_result.get()\n",
    "\n",
    "    for q_result in q_results:\n",
    "        q_result.get()\n",
    "\n",
    "end = time.time()\n",
    "print((end - start)/3600)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
