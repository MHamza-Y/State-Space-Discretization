{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import makedirs\n",
    "from os.path import join, dirname\n",
    "import uuid\n",
    "import torch\n",
    "from functools import partial\n",
    "from papermill import execute_notebook\n",
    "from offline_dataset.multi_modal_dataset import generate_multimodal_dataset\n",
    "import time\n",
    "from multiprocessing.pool import Pool\n",
    "from glob import glob\n",
    "from shutil import rmtree"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initial Setup\n",
    "\n",
    "Create directory structure and initialize parameters for the experiment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "experiment_id = uuid.uuid4().__str__()\n",
    "print(f'Experiment ID: {experiment_id}')\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "experiments_folder_path = join('tmp', 'experiments')\n",
    "root_path = join(experiments_folder_path, experiment_id)\n",
    "notebook_out_path = join(root_path, 'notebook_outputs')\n",
    "tensorboard_dir = join(root_path, 'tensorboard_logs')\n",
    "forecasting_models_root_save_path = join(root_path, 'state_quantization')\n",
    "makedirs(dirname(root_path), exist_ok=True)\n",
    "makedirs(notebook_out_path, exist_ok=True)\n",
    "makedirs(dirname(tensorboard_dir), exist_ok=True)\n",
    "\n",
    "bits = 20\n",
    "model_names = [f'model_h_c-{bits}bits', f'model_aeq-{bits}bits', f'model_final_h-{bits}bits', f'untrained_model_h_c-{bits}bits', f'untrained_model_aeq-{bits}bits', f'untrained_model_final_h-{bits}bits']\n",
    "log_output = True\n",
    "q_learning_epochs = 7000\n",
    "policy_iteration_epochs=100\n",
    "policy_iteration_evaluation_epochs=500\n",
    "r_min_total_epochs=500"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Forecasting Model Training Tasks\n",
    "\n",
    "These tasks execute the notebooks which are responsible for the training of forecasting models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "forecasting_tensorboard_dir = join(tensorboard_dir,'forecasting_models')\n",
    "makedirs(dirname(forecasting_tensorboard_dir), exist_ok=True)\n",
    "train_lstm_ae_notebook = 'train_LSTM-AE.ipynb'\n",
    "\n",
    "train_lstm_ae_task = partial(execute_notebook,\n",
    "\n",
    "                             input_path=train_lstm_ae_notebook,\n",
    "                             output_path=join(notebook_out_path, train_lstm_ae_notebook),\n",
    "                             parameters=\n",
    "                             dict(\n",
    "                                 bits=bits,\n",
    "                                 use_cuda=use_cuda,\n",
    "                                 model_path=join(forecasting_models_root_save_path, f'model_aeq-{bits}bits'),\n",
    "                                 untrained_model_path=join(forecasting_models_root_save_path,\n",
    "                                                           f'untrained_model_aeq-{bits}bits'),\n",
    "                                 log_dir=join(forecasting_tensorboard_dir,f'model_aeq-{bits}bits')\n",
    "                             ),\n",
    "                             log_output=log_output\n",
    "                             )\n",
    "\n",
    "train_discHC_notebook = 'train_DiscHC.ipynb'\n",
    "train_discHC_task = partial(execute_notebook,\n",
    "                            input_path=train_discHC_notebook,\n",
    "                            output_path=join(notebook_out_path, train_discHC_notebook),\n",
    "                            parameters=\n",
    "                            dict(\n",
    "                                bits=bits,\n",
    "                                use_cuda=use_cuda,\n",
    "                                model_path=join(forecasting_models_root_save_path, f'model_h_c-{bits}bits'),\n",
    "                                untrained_model_path=join(forecasting_models_root_save_path,\n",
    "                                                          f'untrained_model_h_c-{bits}bits'),\n",
    "                                model_dict_path=join(forecasting_models_root_save_path, 'model_h_c_dict'),\n",
    "                                log_dir=join(forecasting_tensorboard_dir,f'model_h_c-{bits}bits')\n",
    "                            ),\n",
    "                            log_output=log_output)\n",
    "\n",
    "train_disc_final_h_notebook = 'train_DiscFinalH.ipynb'\n",
    "train_disc_final_h_task = partial(execute_notebook,\n",
    "                                  input_path=train_disc_final_h_notebook,\n",
    "                                  output_path=join(notebook_out_path,\n",
    "                                                   train_disc_final_h_notebook),\n",
    "                                  parameters=\n",
    "                                  dict(\n",
    "                                      bits=bits,\n",
    "                                      use_cuda=use_cuda,\n",
    "                                      model_path=join(forecasting_models_root_save_path, f'model_final_h-{bits}bits'),\n",
    "                                      untrained_model_path=join(forecasting_models_root_save_path,\n",
    "                                                                f'untrained_model_final_h-{bits}bits'),\n",
    "                                      model_dict_path=join(forecasting_models_root_save_path, 'model_final_h_dict'),\n",
    "                                      log_dir=join(forecasting_tensorboard_dir,f'model_final_h-{bits}bits')\n",
    "                                  ),\n",
    "                                  log_output=log_output\n",
    "                                  )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Model Based Q Learning Tasks\n",
    "\n",
    "These tasks run the model based q learning notebooks for each trained and untrained forecasting model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mb_q_learning_tensorboard_dir = join(tensorboard_dir,'mb_q_learning')\n",
    "makedirs(dirname(mb_q_learning_tensorboard_dir), exist_ok=True)\n",
    "train_model_based_q_learning_tasks = []\n",
    "train_model_based_q_learning_task_notebook = 'train_mb_offline_q_learning.ipynb'\n",
    "\n",
    "\n",
    "for model_name in model_names:\n",
    "    train_model_based_q_learning_tasks.append(partial(\n",
    "        execute_notebook,\n",
    "        input_path=train_model_based_q_learning_task_notebook,\n",
    "        output_path=join(notebook_out_path, f'{model_name}_{train_model_based_q_learning_task_notebook}'),\n",
    "        parameters=\n",
    "        dict(\n",
    "            device='cuda' if use_cuda else 'cpu',\n",
    "            root_path=root_path,\n",
    "            total_epochs=q_learning_epochs,\n",
    "            model_name=model_name,\n",
    "            log_dir=join(mb_q_learning_tensorboard_dir,model_name)\n",
    "        ),\n",
    "        log_output=log_output\n",
    "    ))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Q Learning Tasks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q_learning_tensorboard_dir = join(tensorboard_dir,'q_learning')\n",
    "makedirs(dirname(q_learning_tensorboard_dir), exist_ok=True)\n",
    "train_q_learning_tasks = []\n",
    "train_q_learning_task_notebook = 'train_q_learning.ipynb'\n",
    "\n",
    "for model_name in model_names:\n",
    "    train_q_learning_tasks.append(partial(\n",
    "        execute_notebook,\n",
    "        input_path=train_q_learning_task_notebook,\n",
    "        output_path=join(notebook_out_path, f'{model_name}_{train_q_learning_task_notebook}'),\n",
    "        parameters=\n",
    "        dict(\n",
    "            device='cuda' if use_cuda else 'cpu',\n",
    "            root_path=root_path,\n",
    "            total_epochs=q_learning_epochs,\n",
    "            model_name=model_name,\n",
    "            log_dir=join(q_learning_tensorboard_dir,model_name)\n",
    "        ),\n",
    "        log_output=log_output\n",
    "    ))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Policy Iteration Task"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_episodes = [10, 100, 1000, 10000] # the number of episodes to use for offline training and evaluation\n",
    "train_policy_iteration_notebook = 'train_policy_iteration.ipynb'\n",
    "train_policy_iteration_task = partial(\n",
    "    execute_notebook,\n",
    "    input_path=train_policy_iteration_notebook,\n",
    "    output_path=join(notebook_out_path, train_policy_iteration_notebook),\n",
    "    parameters=\n",
    "    dict(\n",
    "        model_names=model_names,\n",
    "        root_path=root_path,\n",
    "        training_episodes=training_episodes,\n",
    "        total_epochs=policy_iteration_epochs,\n",
    "        eval_epochs=policy_iteration_evaluation_epochs\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create R-Min Task"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "min_count = [1, 2, 3, 5] # Min count for each element in training_episodes\n",
    "train_r_min_notebook = 'train_r_min.ipynb'\n",
    "train_r_min_task = partial(\n",
    "    execute_notebook,\n",
    "    input_path=train_r_min_notebook,\n",
    "    output_path=join(notebook_out_path, train_r_min_notebook),\n",
    "    parameters=\n",
    "    dict(\n",
    "        model_names=model_names,\n",
    "        root_path=root_path,\n",
    "        training_episodes=training_episodes,\n",
    "        min_count=min_count,\n",
    "        total_epochs=r_min_total_epochs\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Offline Cleanup Task"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def cleanup_task():\n",
    "    files_to_delete = []\n",
    "    mdp_files_paths = join(experiments_folder_path,'**','mdp','')\n",
    "    rl_trajectories = join(experiments_folder_path,'**','offline_rl_trajectories','')\n",
    "    dataset_creator_tmp_files = join(experiments_folder_path,'**','dataset_creator_tmp','')\n",
    "    files_to_delete.extend(glob(mdp_files_paths))\n",
    "    files_to_delete.extend(glob(rl_trajectories))\n",
    "    files_to_delete.extend(glob(dataset_creator_tmp_files))\n",
    "    for file in files_to_delete:\n",
    "        rmtree(file)\n",
    "    print(files_to_delete)\n",
    "    return files_to_delete\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Execute The Pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "with Pool(3) as pool:\n",
    "    train_lstm_ae_task_result = pool.apply_async(train_lstm_ae_task)\n",
    "    train_discHC_task_result = pool.apply_async(train_discHC_task)\n",
    "    train_disc_final_h_task_result = pool.apply_async(train_disc_final_h_task)\n",
    "\n",
    "    train_lstm_ae_task_result.get()\n",
    "    train_discHC_task_result.get()\n",
    "    train_disc_final_h_task_result.get()\n",
    "\n",
    "    mb_q_results = []\n",
    "    for train_model_based_q_learning_task in train_model_based_q_learning_tasks:\n",
    "        result = pool.apply_async(train_model_based_q_learning_task)\n",
    "        mb_q_results.append(result)\n",
    "\n",
    "    q_results = []\n",
    "    for train_q_learning_task in train_q_learning_tasks:\n",
    "        result = pool.apply_async(train_q_learning_task)\n",
    "        q_results.append(result)\n",
    "\n",
    "    generate_multimodal_dataset(model_names=model_names,episodes=max(training_episodes),root_path=root_path)\n",
    "    train_policy_iteration_result = pool.apply_async(train_policy_iteration_task)\n",
    "    train_r_min_result = pool.apply_async(train_r_min_task)\n",
    "    train_policy_iteration_result.get()\n",
    "    train_r_min_result.get()\n",
    "    cleanup_task_result = pool.apply_async(cleanup_task)\n",
    "    print(f'Cleaned up files: {cleanup_task_result.get()}')\n",
    "\n",
    "    for mb_q_result in mb_q_results:\n",
    "        mb_q_result.get()\n",
    "\n",
    "    for q_result in q_results:\n",
    "        q_result.get()\n",
    "\n",
    "end = time.time()\n",
    "print((end - start)/3600)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
